{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febf9617-68d7-4b74-9681-f5503aa72eb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (3주차-1) 11월26일\n",
    "- 주제: 모델 파이프라인 짜보기\n",
    "- 작성자: 윤도현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6eb8b6c-00ec-41f2-93b2-13f48cd66a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, ToPILImage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89371c-d79c-4163-bd88-d1feb8628f6c",
   "metadata": {},
   "source": [
    "### 이미지 불러와주는 라이브러리 (무시가능) ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70da28d0-705c-4700-b5fb-d777925704b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import *\n",
    "from fastai.vision.all import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dae648-f6cb-40b7-8a45-7bdd0fc6ae09",
   "metadata": {},
   "source": [
    "### torchvision.datasets에서 제공하는 데이터셋 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426e0862-69b7-4089-8710-e1c987347033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "PILImage mode=RGB size=235x873"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PILImage.create('11111111.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524bfaee-5671-462a-8e0a-d81882c0960b",
   "metadata": {},
   "source": [
    "# 1. 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db564b2-f079-4615-9fbb-5c20b55a40b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\bda2021\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# 공개 데이터셋에서 학습 데이터를 내려받습니다.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# 공개 데이터셋에서 테스트 데이터를 내려받습니다.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9d449-1648-45ac-b862-ee8e180266c6",
   "metadata": {},
   "source": [
    "### dataset 훔쳐보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98f5fdcc-f1fc-4c55-8a06-0e8d1c021d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b19ff7fa-54bd-4935-bc88-586a91a59004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
       "         1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7, 6, 7, 2, 1,\n",
       "         2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57e5b6-589a-47e4-8ba4-f2c79b653a28",
   "metadata": {},
   "source": [
    "### datasets.FashionMNIST 인자 훔쳐보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d288a060-cdb3-40c6-a775-6acb4bf597f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFashionMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n",
       "\n",
       "Args:\n",
       "    root (string): Root directory of dataset where ``FashionMNIST/processed/training.pt``\n",
       "        and  ``FashionMNIST/processed/test.pt`` exist.\n",
       "    train (bool, optional): If True, creates dataset from ``training.pt``,\n",
       "        otherwise from ``test.pt``.\n",
       "    download (bool, optional): If true, downloads the dataset from the internet and\n",
       "        puts it in root directory. If dataset is already downloaded, it is not\n",
       "        downloaded again.\n",
       "    transform (callable, optional): A function/transform that  takes in an PIL image\n",
       "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
       "    target_transform (callable, optional): A function/transform that takes in the\n",
       "        target and transforms it.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\anaconda3\\envs\\bda2021\\lib\\site-packages\\torchvision\\datasets\\mnist.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets.FashionMNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108fa9c-af29-49d2-a7c2-54490fad6cc8",
   "metadata": {},
   "source": [
    "### data 폴더 파일목록 훔쳐보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1cdeaac-3beb-42a8-ac13-d8d99fb42a59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: FCD5-95C9\n",
      "\n",
      " C:\\Users\\user\\data 디렉터리\n",
      "\n",
      "2021-11-26  오전 11:05    <DIR>          .\n",
      "2021-11-26  오전 11:05    <DIR>          ..\n",
      "2021-11-26  오전 11:05    <DIR>          FashionMNIST\n",
      "               0개 파일                   0 바이트\n",
      "               3개 디렉터리  1,653,937,725,440 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "ls data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359196b-163a-40d0-9280-7612d9d5aebc",
   "metadata": {},
   "source": [
    "# 2. 불러온 데이터셋으로 데이터 로더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050c5b60-19ee-4625-8bb2-fac3d39a2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# 데이터로더를 생성합니다.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae029f2-5ce8-45bf-a810-17ec597f64f5",
   "metadata": {},
   "source": [
    "# 3. 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c52a8-b411-4e07-bdea-4340b36bc91e",
   "metadata": {},
   "source": [
    "### class를 도입하면 하나의 클래스를 통해 여러개의 객체를 생성 함으로써 \n",
    "\n",
    "### 코드의 반복을 줄일 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de74673e-d56c-4ea3-a9d5-59c23f638f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 학습에 사용할 CPU나 GPU 장치를 얻습니다.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "# layer 정의\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "# 신경망에 데이터를 어떻게 전달할지 정의\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee85fd8-8b21-4da0-9115-acc225f01db5",
   "metadata": {},
   "source": [
    "### nn.Linear 인자 훔쳐보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9c72cf-ca85-4d25-b67c-00365a46a5b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0min_features\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
       "\n",
       "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
       "\n",
       "Args:\n",
       "    in_features: size of each input sample\n",
       "    out_features: size of each output sample\n",
       "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
       "        Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
       "      additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
       "    - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
       "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
       "\n",
       "Attributes:\n",
       "    weight: the learnable weights of the module of shape\n",
       "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
       "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
       "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
       "            If :attr:`bias` is ``True``, the values are initialized from\n",
       "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.Linear(20, 30)\n",
       "    >>> input = torch.randn(128, 20)\n",
       "    >>> output = m(input)\n",
       "    >>> print(output.size())\n",
       "    torch.Size([128, 30])\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\anaconda3\\envs\\bda2021\\lib\\site-packages\\torch\\nn\\modules\\linear.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4917f-4550-4804-9a44-984130b47494",
   "metadata": {},
   "source": [
    "# 4. 손실함수, 옵티마이저 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b238eee7-f661-4478-b2a4-39727f7f1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn =nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr= 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78005b11-8f54-406f-8dcd-417b35421123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1232c801-c3ac-4cd0-ae55-a0ad8d2f94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 예측 오류 계산\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708e654-1ce0-4f56-8e6d-8f8daa706931",
   "metadata": {},
   "source": [
    "### test 데이터셋으로 모델의 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd3664c-44a4-40b3-b6c5-51b5c071092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2897ab-c7cf-4136-968c-ddae7fd74891",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "021777eb-4ef1-4b14-a400-86ecbc1c8f24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.341803  [    0/60000]\n",
      "loss: 1.414391  [ 6400/60000]\n",
      "loss: 1.120190  [12800/60000]\n",
      "loss: 1.121604  [19200/60000]\n",
      "loss: 1.361351  [25600/60000]\n",
      "loss: 1.065126  [32000/60000]\n",
      "loss: 1.411634  [38400/60000]\n",
      "loss: 1.330144  [44800/60000]\n",
      "loss: 1.434133  [51200/60000]\n",
      "loss: 1.288628  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 1.259844 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.340176  [    0/60000]\n",
      "loss: 1.412698  [ 6400/60000]\n",
      "loss: 1.119317  [12800/60000]\n",
      "loss: 1.120391  [19200/60000]\n",
      "loss: 1.359872  [25600/60000]\n",
      "loss: 1.062699  [32000/60000]\n",
      "loss: 1.410056  [38400/60000]\n",
      "loss: 1.329252  [44800/60000]\n",
      "loss: 1.433842  [51200/60000]\n",
      "loss: 1.287258  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 1.258793 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.338544  [    0/60000]\n",
      "loss: 1.411074  [ 6400/60000]\n",
      "loss: 1.118520  [12800/60000]\n",
      "loss: 1.119171  [19200/60000]\n",
      "loss: 1.358127  [25600/60000]\n",
      "loss: 1.060794  [32000/60000]\n",
      "loss: 1.408287  [38400/60000]\n",
      "loss: 1.328419  [44800/60000]\n",
      "loss: 1.433503  [51200/60000]\n",
      "loss: 1.286028  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.257770 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.337008  [    0/60000]\n",
      "loss: 1.409433  [ 6400/60000]\n",
      "loss: 1.117757  [12800/60000]\n",
      "loss: 1.117983  [19200/60000]\n",
      "loss: 1.356377  [25600/60000]\n",
      "loss: 1.058437  [32000/60000]\n",
      "loss: 1.406840  [38400/60000]\n",
      "loss: 1.327379  [44800/60000]\n",
      "loss: 1.432997  [51200/60000]\n",
      "loss: 1.284822  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.256766 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.335543  [    0/60000]\n",
      "loss: 1.407742  [ 6400/60000]\n",
      "loss: 1.116953  [12800/60000]\n",
      "loss: 1.116642  [19200/60000]\n",
      "loss: 1.355007  [25600/60000]\n",
      "loss: 1.056174  [32000/60000]\n",
      "loss: 1.405395  [38400/60000]\n",
      "loss: 1.326420  [44800/60000]\n",
      "loss: 1.432772  [51200/60000]\n",
      "loss: 1.283674  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.255784 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.334080  [    0/60000]\n",
      "loss: 1.406193  [ 6400/60000]\n",
      "loss: 1.116220  [12800/60000]\n",
      "loss: 1.115523  [19200/60000]\n",
      "loss: 1.353585  [25600/60000]\n",
      "loss: 1.054148  [32000/60000]\n",
      "loss: 1.404069  [38400/60000]\n",
      "loss: 1.325474  [44800/60000]\n",
      "loss: 1.432472  [51200/60000]\n",
      "loss: 1.282578  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.254827 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.332631  [    0/60000]\n",
      "loss: 1.404751  [ 6400/60000]\n",
      "loss: 1.115466  [12800/60000]\n",
      "loss: 1.114583  [19200/60000]\n",
      "loss: 1.352275  [25600/60000]\n",
      "loss: 1.051964  [32000/60000]\n",
      "loss: 1.402658  [38400/60000]\n",
      "loss: 1.324558  [44800/60000]\n",
      "loss: 1.432073  [51200/60000]\n",
      "loss: 1.281746  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.253894 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.331267  [    0/60000]\n",
      "loss: 1.403301  [ 6400/60000]\n",
      "loss: 1.114690  [12800/60000]\n",
      "loss: 1.113534  [19200/60000]\n",
      "loss: 1.351100  [25600/60000]\n",
      "loss: 1.049820  [32000/60000]\n",
      "loss: 1.401374  [38400/60000]\n",
      "loss: 1.323651  [44800/60000]\n",
      "loss: 1.431639  [51200/60000]\n",
      "loss: 1.280572  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.252985 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.329965  [    0/60000]\n",
      "loss: 1.401950  [ 6400/60000]\n",
      "loss: 1.113878  [12800/60000]\n",
      "loss: 1.112538  [19200/60000]\n",
      "loss: 1.349904  [25600/60000]\n",
      "loss: 1.047810  [32000/60000]\n",
      "loss: 1.400015  [38400/60000]\n",
      "loss: 1.322690  [44800/60000]\n",
      "loss: 1.431071  [51200/60000]\n",
      "loss: 1.279469  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.252091 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.328684  [    0/60000]\n",
      "loss: 1.400608  [ 6400/60000]\n",
      "loss: 1.113029  [12800/60000]\n",
      "loss: 1.111574  [19200/60000]\n",
      "loss: 1.348690  [25600/60000]\n",
      "loss: 1.045793  [32000/60000]\n",
      "loss: 1.398804  [38400/60000]\n",
      "loss: 1.321713  [44800/60000]\n",
      "loss: 1.430853  [51200/60000]\n",
      "loss: 1.278401  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.251217 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.327367  [    0/60000]\n",
      "loss: 1.399253  [ 6400/60000]\n",
      "loss: 1.112150  [12800/60000]\n",
      "loss: 1.110586  [19200/60000]\n",
      "loss: 1.347517  [25600/60000]\n",
      "loss: 1.043777  [32000/60000]\n",
      "loss: 1.397629  [38400/60000]\n",
      "loss: 1.320642  [44800/60000]\n",
      "loss: 1.430391  [51200/60000]\n",
      "loss: 1.277172  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.250351 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.326110  [    0/60000]\n",
      "loss: 1.397845  [ 6400/60000]\n",
      "loss: 1.111301  [12800/60000]\n",
      "loss: 1.109645  [19200/60000]\n",
      "loss: 1.346337  [25600/60000]\n",
      "loss: 1.041545  [32000/60000]\n",
      "loss: 1.396249  [38400/60000]\n",
      "loss: 1.319731  [44800/60000]\n",
      "loss: 1.429843  [51200/60000]\n",
      "loss: 1.276159  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.249498 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.325047  [    0/60000]\n",
      "loss: 1.396475  [ 6400/60000]\n",
      "loss: 1.110457  [12800/60000]\n",
      "loss: 1.108698  [19200/60000]\n",
      "loss: 1.345109  [25600/60000]\n",
      "loss: 1.039576  [32000/60000]\n",
      "loss: 1.394922  [38400/60000]\n",
      "loss: 1.318744  [44800/60000]\n",
      "loss: 1.429329  [51200/60000]\n",
      "loss: 1.275266  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.248679 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.323874  [    0/60000]\n",
      "loss: 1.395153  [ 6400/60000]\n",
      "loss: 1.109641  [12800/60000]\n",
      "loss: 1.107675  [19200/60000]\n",
      "loss: 1.343834  [25600/60000]\n",
      "loss: 1.037718  [32000/60000]\n",
      "loss: 1.393726  [38400/60000]\n",
      "loss: 1.317821  [44800/60000]\n",
      "loss: 1.428718  [51200/60000]\n",
      "loss: 1.274419  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.247867 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.322773  [    0/60000]\n",
      "loss: 1.393846  [ 6400/60000]\n",
      "loss: 1.108902  [12800/60000]\n",
      "loss: 1.106776  [19200/60000]\n",
      "loss: 1.342484  [25600/60000]\n",
      "loss: 1.035839  [32000/60000]\n",
      "loss: 1.392606  [38400/60000]\n",
      "loss: 1.316802  [44800/60000]\n",
      "loss: 1.428096  [51200/60000]\n",
      "loss: 1.273520  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 1.247068 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.321659  [    0/60000]\n",
      "loss: 1.392704  [ 6400/60000]\n",
      "loss: 1.108107  [12800/60000]\n",
      "loss: 1.105801  [19200/60000]\n",
      "loss: 1.341195  [25600/60000]\n",
      "loss: 1.034066  [32000/60000]\n",
      "loss: 1.391377  [38400/60000]\n",
      "loss: 1.315858  [44800/60000]\n",
      "loss: 1.427476  [51200/60000]\n",
      "loss: 1.272693  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 1.246301 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.320584  [    0/60000]\n",
      "loss: 1.391491  [ 6400/60000]\n",
      "loss: 1.107325  [12800/60000]\n",
      "loss: 1.104797  [19200/60000]\n",
      "loss: 1.339993  [25600/60000]\n",
      "loss: 1.032155  [32000/60000]\n",
      "loss: 1.390310  [38400/60000]\n",
      "loss: 1.314899  [44800/60000]\n",
      "loss: 1.426827  [51200/60000]\n",
      "loss: 1.271876  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.245552 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.319546  [    0/60000]\n",
      "loss: 1.390296  [ 6400/60000]\n",
      "loss: 1.106590  [12800/60000]\n",
      "loss: 1.103833  [19200/60000]\n",
      "loss: 1.338572  [25600/60000]\n",
      "loss: 1.030343  [32000/60000]\n",
      "loss: 1.389160  [38400/60000]\n",
      "loss: 1.314056  [44800/60000]\n",
      "loss: 1.426201  [51200/60000]\n",
      "loss: 1.271106  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.244815 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.318512  [    0/60000]\n",
      "loss: 1.389093  [ 6400/60000]\n",
      "loss: 1.105889  [12800/60000]\n",
      "loss: 1.102908  [19200/60000]\n",
      "loss: 1.337307  [25600/60000]\n",
      "loss: 1.028603  [32000/60000]\n",
      "loss: 1.388053  [38400/60000]\n",
      "loss: 1.313016  [44800/60000]\n",
      "loss: 1.425605  [51200/60000]\n",
      "loss: 1.270185  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.244074 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.317449  [    0/60000]\n",
      "loss: 1.387936  [ 6400/60000]\n",
      "loss: 1.105184  [12800/60000]\n",
      "loss: 1.102014  [19200/60000]\n",
      "loss: 1.336091  [25600/60000]\n",
      "loss: 1.026850  [32000/60000]\n",
      "loss: 1.386905  [38400/60000]\n",
      "loss: 1.312115  [44800/60000]\n",
      "loss: 1.425064  [51200/60000]\n",
      "loss: 1.269374  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.243348 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.316453  [    0/60000]\n",
      "loss: 1.386718  [ 6400/60000]\n",
      "loss: 1.104379  [12800/60000]\n",
      "loss: 1.101040  [19200/60000]\n",
      "loss: 1.335028  [25600/60000]\n",
      "loss: 1.025059  [32000/60000]\n",
      "loss: 1.385843  [38400/60000]\n",
      "loss: 1.311244  [44800/60000]\n",
      "loss: 1.424245  [51200/60000]\n",
      "loss: 1.268611  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.242645 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.315519  [    0/60000]\n",
      "loss: 1.385572  [ 6400/60000]\n",
      "loss: 1.103683  [12800/60000]\n",
      "loss: 1.100214  [19200/60000]\n",
      "loss: 1.333969  [25600/60000]\n",
      "loss: 1.023287  [32000/60000]\n",
      "loss: 1.384845  [38400/60000]\n",
      "loss: 1.310358  [44800/60000]\n",
      "loss: 1.423684  [51200/60000]\n",
      "loss: 1.268059  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.241954 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.314587  [    0/60000]\n",
      "loss: 1.384380  [ 6400/60000]\n",
      "loss: 1.103052  [12800/60000]\n",
      "loss: 1.099349  [19200/60000]\n",
      "loss: 1.333085  [25600/60000]\n",
      "loss: 1.021663  [32000/60000]\n",
      "loss: 1.383885  [38400/60000]\n",
      "loss: 1.309534  [44800/60000]\n",
      "loss: 1.423021  [51200/60000]\n",
      "loss: 1.267367  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.241275 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.313697  [    0/60000]\n",
      "loss: 1.383226  [ 6400/60000]\n",
      "loss: 1.102296  [12800/60000]\n",
      "loss: 1.098538  [19200/60000]\n",
      "loss: 1.332100  [25600/60000]\n",
      "loss: 1.020082  [32000/60000]\n",
      "loss: 1.382890  [38400/60000]\n",
      "loss: 1.308676  [44800/60000]\n",
      "loss: 1.422465  [51200/60000]\n",
      "loss: 1.266667  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.240611 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.312783  [    0/60000]\n",
      "loss: 1.382113  [ 6400/60000]\n",
      "loss: 1.101625  [12800/60000]\n",
      "loss: 1.097674  [19200/60000]\n",
      "loss: 1.330994  [25600/60000]\n",
      "loss: 1.018512  [32000/60000]\n",
      "loss: 1.381870  [38400/60000]\n",
      "loss: 1.307784  [44800/60000]\n",
      "loss: 1.421880  [51200/60000]\n",
      "loss: 1.266017  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.239957 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.311884  [    0/60000]\n",
      "loss: 1.381019  [ 6400/60000]\n",
      "loss: 1.100913  [12800/60000]\n",
      "loss: 1.096803  [19200/60000]\n",
      "loss: 1.330032  [25600/60000]\n",
      "loss: 1.017059  [32000/60000]\n",
      "loss: 1.380647  [38400/60000]\n",
      "loss: 1.307075  [44800/60000]\n",
      "loss: 1.421278  [51200/60000]\n",
      "loss: 1.265409  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.239310 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.311067  [    0/60000]\n",
      "loss: 1.379952  [ 6400/60000]\n",
      "loss: 1.100207  [12800/60000]\n",
      "loss: 1.096031  [19200/60000]\n",
      "loss: 1.328809  [25600/60000]\n",
      "loss: 1.015008  [32000/60000]\n",
      "loss: 1.379568  [38400/60000]\n",
      "loss: 1.306356  [44800/60000]\n",
      "loss: 1.420574  [51200/60000]\n",
      "loss: 1.264819  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.238665 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.310317  [    0/60000]\n",
      "loss: 1.378905  [ 6400/60000]\n",
      "loss: 1.099495  [12800/60000]\n",
      "loss: 1.095288  [19200/60000]\n",
      "loss: 1.327722  [25600/60000]\n",
      "loss: 1.013482  [32000/60000]\n",
      "loss: 1.378615  [38400/60000]\n",
      "loss: 1.305523  [44800/60000]\n",
      "loss: 1.420013  [51200/60000]\n",
      "loss: 1.264197  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.238029 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.309535  [    0/60000]\n",
      "loss: 1.377856  [ 6400/60000]\n",
      "loss: 1.098795  [12800/60000]\n",
      "loss: 1.094352  [19200/60000]\n",
      "loss: 1.326570  [25600/60000]\n",
      "loss: 1.012017  [32000/60000]\n",
      "loss: 1.377642  [38400/60000]\n",
      "loss: 1.304723  [44800/60000]\n",
      "loss: 1.419416  [51200/60000]\n",
      "loss: 1.263513  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.237406 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.308723  [    0/60000]\n",
      "loss: 1.377002  [ 6400/60000]\n",
      "loss: 1.098203  [12800/60000]\n",
      "loss: 1.093598  [19200/60000]\n",
      "loss: 1.325480  [25600/60000]\n",
      "loss: 1.010413  [32000/60000]\n",
      "loss: 1.376711  [38400/60000]\n",
      "loss: 1.303851  [44800/60000]\n",
      "loss: 1.418798  [51200/60000]\n",
      "loss: 1.262865  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.236802 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.307981  [    0/60000]\n",
      "loss: 1.376037  [ 6400/60000]\n",
      "loss: 1.097607  [12800/60000]\n",
      "loss: 1.092910  [19200/60000]\n",
      "loss: 1.324542  [25600/60000]\n",
      "loss: 1.008943  [32000/60000]\n",
      "loss: 1.375726  [38400/60000]\n",
      "loss: 1.303084  [44800/60000]\n",
      "loss: 1.418210  [51200/60000]\n",
      "loss: 1.262239  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.236197 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.307247  [    0/60000]\n",
      "loss: 1.375101  [ 6400/60000]\n",
      "loss: 1.096897  [12800/60000]\n",
      "loss: 1.092205  [19200/60000]\n",
      "loss: 1.323451  [25600/60000]\n",
      "loss: 1.007468  [32000/60000]\n",
      "loss: 1.374748  [38400/60000]\n",
      "loss: 1.302295  [44800/60000]\n",
      "loss: 1.417499  [51200/60000]\n",
      "loss: 1.261643  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.235590 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.306510  [    0/60000]\n",
      "loss: 1.374327  [ 6400/60000]\n",
      "loss: 1.096281  [12800/60000]\n",
      "loss: 1.091456  [19200/60000]\n",
      "loss: 1.322380  [25600/60000]\n",
      "loss: 1.006112  [32000/60000]\n",
      "loss: 1.373697  [38400/60000]\n",
      "loss: 1.301399  [44800/60000]\n",
      "loss: 1.416568  [51200/60000]\n",
      "loss: 1.261042  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.234988 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.305859  [    0/60000]\n",
      "loss: 1.373687  [ 6400/60000]\n",
      "loss: 1.095753  [12800/60000]\n",
      "loss: 1.090797  [19200/60000]\n",
      "loss: 1.321404  [25600/60000]\n",
      "loss: 1.004756  [32000/60000]\n",
      "loss: 1.372732  [38400/60000]\n",
      "loss: 1.300549  [44800/60000]\n",
      "loss: 1.415624  [51200/60000]\n",
      "loss: 1.260411  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.234402 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.305246  [    0/60000]\n",
      "loss: 1.372827  [ 6400/60000]\n",
      "loss: 1.095077  [12800/60000]\n",
      "loss: 1.090126  [19200/60000]\n",
      "loss: 1.320409  [25600/60000]\n",
      "loss: 1.003440  [32000/60000]\n",
      "loss: 1.371888  [38400/60000]\n",
      "loss: 1.299668  [44800/60000]\n",
      "loss: 1.414671  [51200/60000]\n",
      "loss: 1.259946  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.233832 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.304655  [    0/60000]\n",
      "loss: 1.371935  [ 6400/60000]\n",
      "loss: 1.094425  [12800/60000]\n",
      "loss: 1.089520  [19200/60000]\n",
      "loss: 1.319347  [25600/60000]\n",
      "loss: 1.002144  [32000/60000]\n",
      "loss: 1.370947  [38400/60000]\n",
      "loss: 1.298770  [44800/60000]\n",
      "loss: 1.413914  [51200/60000]\n",
      "loss: 1.259346  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.233264 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.303995  [    0/60000]\n",
      "loss: 1.371115  [ 6400/60000]\n",
      "loss: 1.093830  [12800/60000]\n",
      "loss: 1.088870  [19200/60000]\n",
      "loss: 1.318292  [25600/60000]\n",
      "loss: 1.000872  [32000/60000]\n",
      "loss: 1.370045  [38400/60000]\n",
      "loss: 1.297916  [44800/60000]\n",
      "loss: 1.413096  [51200/60000]\n",
      "loss: 1.258751  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.232696 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.303401  [    0/60000]\n",
      "loss: 1.370241  [ 6400/60000]\n",
      "loss: 1.093204  [12800/60000]\n",
      "loss: 1.088266  [19200/60000]\n",
      "loss: 1.317107  [25600/60000]\n",
      "loss: 0.999610  [32000/60000]\n",
      "loss: 1.369131  [38400/60000]\n",
      "loss: 1.297062  [44800/60000]\n",
      "loss: 1.412253  [51200/60000]\n",
      "loss: 1.258186  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.232138 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.302789  [    0/60000]\n",
      "loss: 1.369440  [ 6400/60000]\n",
      "loss: 1.092509  [12800/60000]\n",
      "loss: 1.087640  [19200/60000]\n",
      "loss: 1.316145  [25600/60000]\n",
      "loss: 0.998424  [32000/60000]\n",
      "loss: 1.368216  [38400/60000]\n",
      "loss: 1.296453  [44800/60000]\n",
      "loss: 1.411434  [51200/60000]\n",
      "loss: 1.257646  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.231595 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.302179  [    0/60000]\n",
      "loss: 1.368713  [ 6400/60000]\n",
      "loss: 1.091996  [12800/60000]\n",
      "loss: 1.087065  [19200/60000]\n",
      "loss: 1.315230  [25600/60000]\n",
      "loss: 0.997101  [32000/60000]\n",
      "loss: 1.367317  [38400/60000]\n",
      "loss: 1.295504  [44800/60000]\n",
      "loss: 1.410672  [51200/60000]\n",
      "loss: 1.257082  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.231016 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.301647  [    0/60000]\n",
      "loss: 1.367985  [ 6400/60000]\n",
      "loss: 1.091406  [12800/60000]\n",
      "loss: 1.086468  [19200/60000]\n",
      "loss: 1.314239  [25600/60000]\n",
      "loss: 0.995955  [32000/60000]\n",
      "loss: 1.366436  [38400/60000]\n",
      "loss: 1.294634  [44800/60000]\n",
      "loss: 1.409737  [51200/60000]\n",
      "loss: 1.256556  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.230459 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.301173  [    0/60000]\n",
      "loss: 1.367193  [ 6400/60000]\n",
      "loss: 1.090833  [12800/60000]\n",
      "loss: 1.085845  [19200/60000]\n",
      "loss: 1.313281  [25600/60000]\n",
      "loss: 0.994699  [32000/60000]\n",
      "loss: 1.365584  [38400/60000]\n",
      "loss: 1.293856  [44800/60000]\n",
      "loss: 1.408941  [51200/60000]\n",
      "loss: 1.256027  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.229931 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.300597  [    0/60000]\n",
      "loss: 1.366507  [ 6400/60000]\n",
      "loss: 1.090354  [12800/60000]\n",
      "loss: 1.085288  [19200/60000]\n",
      "loss: 1.312304  [25600/60000]\n",
      "loss: 0.993448  [32000/60000]\n",
      "loss: 1.364657  [38400/60000]\n",
      "loss: 1.293089  [44800/60000]\n",
      "loss: 1.408215  [51200/60000]\n",
      "loss: 1.255527  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.229264 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.300317  [    0/60000]\n",
      "loss: 1.365817  [ 6400/60000]\n",
      "loss: 1.089782  [12800/60000]\n",
      "loss: 1.084674  [19200/60000]\n",
      "loss: 1.310985  [25600/60000]\n",
      "loss: 0.992335  [32000/60000]\n",
      "loss: 1.363877  [38400/60000]\n",
      "loss: 1.292256  [44800/60000]\n",
      "loss: 1.407447  [51200/60000]\n",
      "loss: 1.255031  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.228742 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.299751  [    0/60000]\n",
      "loss: 1.365136  [ 6400/60000]\n",
      "loss: 1.089200  [12800/60000]\n",
      "loss: 1.084023  [19200/60000]\n",
      "loss: 1.310072  [25600/60000]\n",
      "loss: 0.991146  [32000/60000]\n",
      "loss: 1.362970  [38400/60000]\n",
      "loss: 1.291540  [44800/60000]\n",
      "loss: 1.406608  [51200/60000]\n",
      "loss: 1.254511  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.228219 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.299254  [    0/60000]\n",
      "loss: 1.364548  [ 6400/60000]\n",
      "loss: 1.088561  [12800/60000]\n",
      "loss: 1.083463  [19200/60000]\n",
      "loss: 1.309168  [25600/60000]\n",
      "loss: 0.989911  [32000/60000]\n",
      "loss: 1.362093  [38400/60000]\n",
      "loss: 1.290965  [44800/60000]\n",
      "loss: 1.405826  [51200/60000]\n",
      "loss: 1.254013  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.227707 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.298797  [    0/60000]\n",
      "loss: 1.363910  [ 6400/60000]\n",
      "loss: 1.088022  [12800/60000]\n",
      "loss: 1.082852  [19200/60000]\n",
      "loss: 1.307970  [25600/60000]\n",
      "loss: 0.988593  [32000/60000]\n",
      "loss: 1.361259  [38400/60000]\n",
      "loss: 1.290318  [44800/60000]\n",
      "loss: 1.404983  [51200/60000]\n",
      "loss: 1.253595  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.227206 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.298328  [    0/60000]\n",
      "loss: 1.363235  [ 6400/60000]\n",
      "loss: 1.087400  [12800/60000]\n",
      "loss: 1.082240  [19200/60000]\n",
      "loss: 1.307120  [25600/60000]\n",
      "loss: 0.987491  [32000/60000]\n",
      "loss: 1.360435  [38400/60000]\n",
      "loss: 1.289724  [44800/60000]\n",
      "loss: 1.404179  [51200/60000]\n",
      "loss: 1.253119  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.226711 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.297852  [    0/60000]\n",
      "loss: 1.362597  [ 6400/60000]\n",
      "loss: 1.086800  [12800/60000]\n",
      "loss: 1.081592  [19200/60000]\n",
      "loss: 1.306282  [25600/60000]\n",
      "loss: 0.986414  [32000/60000]\n",
      "loss: 1.359633  [38400/60000]\n",
      "loss: 1.289157  [44800/60000]\n",
      "loss: 1.403386  [51200/60000]\n",
      "loss: 1.252622  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.226219 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.297430  [    0/60000]\n",
      "loss: 1.361937  [ 6400/60000]\n",
      "loss: 1.086215  [12800/60000]\n",
      "loss: 1.080983  [19200/60000]\n",
      "loss: 1.305405  [25600/60000]\n",
      "loss: 0.985260  [32000/60000]\n",
      "loss: 1.358859  [38400/60000]\n",
      "loss: 1.288503  [44800/60000]\n",
      "loss: 1.402592  [51200/60000]\n",
      "loss: 1.252134  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.225734 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.296998  [    0/60000]\n",
      "loss: 1.361309  [ 6400/60000]\n",
      "loss: 1.085673  [12800/60000]\n",
      "loss: 1.080405  [19200/60000]\n",
      "loss: 1.304512  [25600/60000]\n",
      "loss: 0.984056  [32000/60000]\n",
      "loss: 1.358010  [38400/60000]\n",
      "loss: 1.287949  [44800/60000]\n",
      "loss: 1.401766  [51200/60000]\n",
      "loss: 1.251634  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.225251 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.296596  [    0/60000]\n",
      "loss: 1.360684  [ 6400/60000]\n",
      "loss: 1.085115  [12800/60000]\n",
      "loss: 1.079927  [19200/60000]\n",
      "loss: 1.303547  [25600/60000]\n",
      "loss: 0.982968  [32000/60000]\n",
      "loss: 1.357241  [38400/60000]\n",
      "loss: 1.287301  [44800/60000]\n",
      "loss: 1.401004  [51200/60000]\n",
      "loss: 1.251135  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.224779 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.296150  [    0/60000]\n",
      "loss: 1.360047  [ 6400/60000]\n",
      "loss: 1.084564  [12800/60000]\n",
      "loss: 1.079351  [19200/60000]\n",
      "loss: 1.302657  [25600/60000]\n",
      "loss: 0.981906  [32000/60000]\n",
      "loss: 1.356448  [38400/60000]\n",
      "loss: 1.286604  [44800/60000]\n",
      "loss: 1.400269  [51200/60000]\n",
      "loss: 1.250482  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.224296 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.295748  [    0/60000]\n",
      "loss: 1.359432  [ 6400/60000]\n",
      "loss: 1.084036  [12800/60000]\n",
      "loss: 1.078799  [19200/60000]\n",
      "loss: 1.301692  [25600/60000]\n",
      "loss: 0.980909  [32000/60000]\n",
      "loss: 1.355634  [38400/60000]\n",
      "loss: 1.285943  [44800/60000]\n",
      "loss: 1.399504  [51200/60000]\n",
      "loss: 1.249992  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.223827 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.295401  [    0/60000]\n",
      "loss: 1.358833  [ 6400/60000]\n",
      "loss: 1.083503  [12800/60000]\n",
      "loss: 1.078232  [19200/60000]\n",
      "loss: 1.300765  [25600/60000]\n",
      "loss: 0.979877  [32000/60000]\n",
      "loss: 1.354881  [38400/60000]\n",
      "loss: 1.285208  [44800/60000]\n",
      "loss: 1.398761  [51200/60000]\n",
      "loss: 1.249460  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.223362 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.295035  [    0/60000]\n",
      "loss: 1.358208  [ 6400/60000]\n",
      "loss: 1.082990  [12800/60000]\n",
      "loss: 1.077696  [19200/60000]\n",
      "loss: 1.299814  [25600/60000]\n",
      "loss: 0.978920  [32000/60000]\n",
      "loss: 1.354108  [38400/60000]\n",
      "loss: 1.284602  [44800/60000]\n",
      "loss: 1.397995  [51200/60000]\n",
      "loss: 1.248911  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.222899 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.294655  [    0/60000]\n",
      "loss: 1.357620  [ 6400/60000]\n",
      "loss: 1.082425  [12800/60000]\n",
      "loss: 1.077131  [19200/60000]\n",
      "loss: 1.298925  [25600/60000]\n",
      "loss: 0.977963  [32000/60000]\n",
      "loss: 1.353397  [38400/60000]\n",
      "loss: 1.283990  [44800/60000]\n",
      "loss: 1.397261  [51200/60000]\n",
      "loss: 1.248492  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.222444 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.294333  [    0/60000]\n",
      "loss: 1.357028  [ 6400/60000]\n",
      "loss: 1.081901  [12800/60000]\n",
      "loss: 1.076517  [19200/60000]\n",
      "loss: 1.298041  [25600/60000]\n",
      "loss: 0.977015  [32000/60000]\n",
      "loss: 1.352654  [38400/60000]\n",
      "loss: 1.283725  [44800/60000]\n",
      "loss: 1.396531  [51200/60000]\n",
      "loss: 1.247803  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.221966 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.294020  [    0/60000]\n",
      "loss: 1.356538  [ 6400/60000]\n",
      "loss: 1.081368  [12800/60000]\n",
      "loss: 1.075936  [19200/60000]\n",
      "loss: 1.297193  [25600/60000]\n",
      "loss: 0.976087  [32000/60000]\n",
      "loss: 1.351855  [38400/60000]\n",
      "loss: 1.283194  [44800/60000]\n",
      "loss: 1.395744  [51200/60000]\n",
      "loss: 1.247351  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.221516 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.293721  [    0/60000]\n",
      "loss: 1.355979  [ 6400/60000]\n",
      "loss: 1.080853  [12800/60000]\n",
      "loss: 1.075356  [19200/60000]\n",
      "loss: 1.296264  [25600/60000]\n",
      "loss: 0.975069  [32000/60000]\n",
      "loss: 1.351108  [38400/60000]\n",
      "loss: 1.282509  [44800/60000]\n",
      "loss: 1.395018  [51200/60000]\n",
      "loss: 1.246906  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.221070 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.293393  [    0/60000]\n",
      "loss: 1.355378  [ 6400/60000]\n",
      "loss: 1.080346  [12800/60000]\n",
      "loss: 1.074744  [19200/60000]\n",
      "loss: 1.295142  [25600/60000]\n",
      "loss: 0.974071  [32000/60000]\n",
      "loss: 1.350417  [38400/60000]\n",
      "loss: 1.281806  [44800/60000]\n",
      "loss: 1.394366  [51200/60000]\n",
      "loss: 1.246493  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.220645 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.293073  [    0/60000]\n",
      "loss: 1.354820  [ 6400/60000]\n",
      "loss: 1.079808  [12800/60000]\n",
      "loss: 1.074207  [19200/60000]\n",
      "loss: 1.294293  [25600/60000]\n",
      "loss: 0.973139  [32000/60000]\n",
      "loss: 1.349674  [38400/60000]\n",
      "loss: 1.281227  [44800/60000]\n",
      "loss: 1.393573  [51200/60000]\n",
      "loss: 1.246035  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.220195 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.292744  [    0/60000]\n",
      "loss: 1.354300  [ 6400/60000]\n",
      "loss: 1.079141  [12800/60000]\n",
      "loss: 1.073674  [19200/60000]\n",
      "loss: 1.293353  [25600/60000]\n",
      "loss: 0.972305  [32000/60000]\n",
      "loss: 1.348891  [38400/60000]\n",
      "loss: 1.280657  [44800/60000]\n",
      "loss: 1.392751  [51200/60000]\n",
      "loss: 1.245591  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.219755 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.292447  [    0/60000]\n",
      "loss: 1.353762  [ 6400/60000]\n",
      "loss: 1.078561  [12800/60000]\n",
      "loss: 1.073033  [19200/60000]\n",
      "loss: 1.292512  [25600/60000]\n",
      "loss: 0.971327  [32000/60000]\n",
      "loss: 1.348143  [38400/60000]\n",
      "loss: 1.280224  [44800/60000]\n",
      "loss: 1.391703  [51200/60000]\n",
      "loss: 1.245142  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.219310 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.292188  [    0/60000]\n",
      "loss: 1.353167  [ 6400/60000]\n",
      "loss: 1.078006  [12800/60000]\n",
      "loss: 1.072475  [19200/60000]\n",
      "loss: 1.291551  [25600/60000]\n",
      "loss: 0.970630  [32000/60000]\n",
      "loss: 1.347371  [38400/60000]\n",
      "loss: 1.279630  [44800/60000]\n",
      "loss: 1.390934  [51200/60000]\n",
      "loss: 1.244752  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.218886 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.291901  [    0/60000]\n",
      "loss: 1.352634  [ 6400/60000]\n",
      "loss: 1.077497  [12800/60000]\n",
      "loss: 1.071934  [19200/60000]\n",
      "loss: 1.290670  [25600/60000]\n",
      "loss: 0.969780  [32000/60000]\n",
      "loss: 1.346607  [38400/60000]\n",
      "loss: 1.279093  [44800/60000]\n",
      "loss: 1.390222  [51200/60000]\n",
      "loss: 1.244300  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.218459 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.291635  [    0/60000]\n",
      "loss: 1.352170  [ 6400/60000]\n",
      "loss: 1.076984  [12800/60000]\n",
      "loss: 1.071375  [19200/60000]\n",
      "loss: 1.289857  [25600/60000]\n",
      "loss: 0.968992  [32000/60000]\n",
      "loss: 1.345863  [38400/60000]\n",
      "loss: 1.278572  [44800/60000]\n",
      "loss: 1.389487  [51200/60000]\n",
      "loss: 1.243804  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.218041 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.291335  [    0/60000]\n",
      "loss: 1.351661  [ 6400/60000]\n",
      "loss: 1.076445  [12800/60000]\n",
      "loss: 1.070782  [19200/60000]\n",
      "loss: 1.289073  [25600/60000]\n",
      "loss: 0.968071  [32000/60000]\n",
      "loss: 1.345204  [38400/60000]\n",
      "loss: 1.278107  [44800/60000]\n",
      "loss: 1.388776  [51200/60000]\n",
      "loss: 1.243326  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.217632 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.291046  [    0/60000]\n",
      "loss: 1.351147  [ 6400/60000]\n",
      "loss: 1.075824  [12800/60000]\n",
      "loss: 1.070188  [19200/60000]\n",
      "loss: 1.288208  [25600/60000]\n",
      "loss: 0.967250  [32000/60000]\n",
      "loss: 1.344533  [38400/60000]\n",
      "loss: 1.277664  [44800/60000]\n",
      "loss: 1.388045  [51200/60000]\n",
      "loss: 1.242835  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.217222 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.290766  [    0/60000]\n",
      "loss: 1.350659  [ 6400/60000]\n",
      "loss: 1.075342  [12800/60000]\n",
      "loss: 1.069569  [19200/60000]\n",
      "loss: 1.287368  [25600/60000]\n",
      "loss: 0.966435  [32000/60000]\n",
      "loss: 1.343894  [38400/60000]\n",
      "loss: 1.277650  [44800/60000]\n",
      "loss: 1.387345  [51200/60000]\n",
      "loss: 1.242287  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.216813 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.290447  [    0/60000]\n",
      "loss: 1.350154  [ 6400/60000]\n",
      "loss: 1.074831  [12800/60000]\n",
      "loss: 1.068978  [19200/60000]\n",
      "loss: 1.286536  [25600/60000]\n",
      "loss: 0.965644  [32000/60000]\n",
      "loss: 1.343200  [38400/60000]\n",
      "loss: 1.277139  [44800/60000]\n",
      "loss: 1.386616  [51200/60000]\n",
      "loss: 1.241830  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.216410 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.290159  [    0/60000]\n",
      "loss: 1.349667  [ 6400/60000]\n",
      "loss: 1.074321  [12800/60000]\n",
      "loss: 1.068431  [19200/60000]\n",
      "loss: 1.285698  [25600/60000]\n",
      "loss: 0.964896  [32000/60000]\n",
      "loss: 1.342487  [38400/60000]\n",
      "loss: 1.276736  [44800/60000]\n",
      "loss: 1.385910  [51200/60000]\n",
      "loss: 1.241355  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.216011 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.289913  [    0/60000]\n",
      "loss: 1.349198  [ 6400/60000]\n",
      "loss: 1.073818  [12800/60000]\n",
      "loss: 1.067846  [19200/60000]\n",
      "loss: 1.284861  [25600/60000]\n",
      "loss: 0.964118  [32000/60000]\n",
      "loss: 1.341755  [38400/60000]\n",
      "loss: 1.276281  [44800/60000]\n",
      "loss: 1.385183  [51200/60000]\n",
      "loss: 1.240932  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.215610 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.289651  [    0/60000]\n",
      "loss: 1.348712  [ 6400/60000]\n",
      "loss: 1.073330  [12800/60000]\n",
      "loss: 1.067277  [19200/60000]\n",
      "loss: 1.283944  [25600/60000]\n",
      "loss: 0.963981  [32000/60000]\n",
      "loss: 1.340997  [38400/60000]\n",
      "loss: 1.275789  [44800/60000]\n",
      "loss: 1.384570  [51200/60000]\n",
      "loss: 1.240574  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.215216 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.289465  [    0/60000]\n",
      "loss: 1.348207  [ 6400/60000]\n",
      "loss: 1.072856  [12800/60000]\n",
      "loss: 1.066702  [19200/60000]\n",
      "loss: 1.283180  [25600/60000]\n",
      "loss: 0.963056  [32000/60000]\n",
      "loss: 1.340621  [38400/60000]\n",
      "loss: 1.275213  [44800/60000]\n",
      "loss: 1.383915  [51200/60000]\n",
      "loss: 1.240166  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.214824 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.289232  [    0/60000]\n",
      "loss: 1.347890  [ 6400/60000]\n",
      "loss: 1.072385  [12800/60000]\n",
      "loss: 1.066087  [19200/60000]\n",
      "loss: 1.282400  [25600/60000]\n",
      "loss: 0.962325  [32000/60000]\n",
      "loss: 1.339902  [38400/60000]\n",
      "loss: 1.274631  [44800/60000]\n",
      "loss: 1.383258  [51200/60000]\n",
      "loss: 1.239766  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.214428 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.289051  [    0/60000]\n",
      "loss: 1.347351  [ 6400/60000]\n",
      "loss: 1.071928  [12800/60000]\n",
      "loss: 1.065478  [19200/60000]\n",
      "loss: 1.281595  [25600/60000]\n",
      "loss: 0.961652  [32000/60000]\n",
      "loss: 1.339244  [38400/60000]\n",
      "loss: 1.274079  [44800/60000]\n",
      "loss: 1.382542  [51200/60000]\n",
      "loss: 1.239395  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.214036 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.288794  [    0/60000]\n",
      "loss: 1.346865  [ 6400/60000]\n",
      "loss: 1.071446  [12800/60000]\n",
      "loss: 1.064832  [19200/60000]\n",
      "loss: 1.280775  [25600/60000]\n",
      "loss: 0.960884  [32000/60000]\n",
      "loss: 1.338426  [38400/60000]\n",
      "loss: 1.273594  [44800/60000]\n",
      "loss: 1.381799  [51200/60000]\n",
      "loss: 1.239095  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.213648 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.288616  [    0/60000]\n",
      "loss: 1.346363  [ 6400/60000]\n",
      "loss: 1.070950  [12800/60000]\n",
      "loss: 1.064257  [19200/60000]\n",
      "loss: 1.279947  [25600/60000]\n",
      "loss: 0.960992  [32000/60000]\n",
      "loss: 1.337711  [38400/60000]\n",
      "loss: 1.272968  [44800/60000]\n",
      "loss: 1.381093  [51200/60000]\n",
      "loss: 1.238721  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.213270 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.288401  [    0/60000]\n",
      "loss: 1.345951  [ 6400/60000]\n",
      "loss: 1.070396  [12800/60000]\n",
      "loss: 1.063688  [19200/60000]\n",
      "loss: 1.279141  [25600/60000]\n",
      "loss: 0.960244  [32000/60000]\n",
      "loss: 1.337030  [38400/60000]\n",
      "loss: 1.272318  [44800/60000]\n",
      "loss: 1.380385  [51200/60000]\n",
      "loss: 1.238332  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.212889 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.288205  [    0/60000]\n",
      "loss: 1.345518  [ 6400/60000]\n",
      "loss: 1.069902  [12800/60000]\n",
      "loss: 1.063142  [19200/60000]\n",
      "loss: 1.278326  [25600/60000]\n",
      "loss: 0.959527  [32000/60000]\n",
      "loss: 1.336301  [38400/60000]\n",
      "loss: 1.271778  [44800/60000]\n",
      "loss: 1.379643  [51200/60000]\n",
      "loss: 1.237903  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.212516 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.287978  [    0/60000]\n",
      "loss: 1.345077  [ 6400/60000]\n",
      "loss: 1.069428  [12800/60000]\n",
      "loss: 1.062564  [19200/60000]\n",
      "loss: 1.277529  [25600/60000]\n",
      "loss: 0.958856  [32000/60000]\n",
      "loss: 1.335568  [38400/60000]\n",
      "loss: 1.271254  [44800/60000]\n",
      "loss: 1.378907  [51200/60000]\n",
      "loss: 1.237521  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.212148 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.287782  [    0/60000]\n",
      "loss: 1.344953  [ 6400/60000]\n",
      "loss: 1.068941  [12800/60000]\n",
      "loss: 1.061965  [19200/60000]\n",
      "loss: 1.276767  [25600/60000]\n",
      "loss: 0.958058  [32000/60000]\n",
      "loss: 1.334834  [38400/60000]\n",
      "loss: 1.270734  [44800/60000]\n",
      "loss: 1.377971  [51200/60000]\n",
      "loss: 1.237076  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.211769 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.287618  [    0/60000]\n",
      "loss: 1.344516  [ 6400/60000]\n",
      "loss: 1.068450  [12800/60000]\n",
      "loss: 1.061383  [19200/60000]\n",
      "loss: 1.275997  [25600/60000]\n",
      "loss: 0.957301  [32000/60000]\n",
      "loss: 1.334196  [38400/60000]\n",
      "loss: 1.270248  [44800/60000]\n",
      "loss: 1.377314  [51200/60000]\n",
      "loss: 1.236681  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.211400 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.287438  [    0/60000]\n",
      "loss: 1.344069  [ 6400/60000]\n",
      "loss: 1.067958  [12800/60000]\n",
      "loss: 1.060789  [19200/60000]\n",
      "loss: 1.275167  [25600/60000]\n",
      "loss: 0.956647  [32000/60000]\n",
      "loss: 1.333488  [38400/60000]\n",
      "loss: 1.269687  [44800/60000]\n",
      "loss: 1.376624  [51200/60000]\n",
      "loss: 1.236262  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.211031 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.287269  [    0/60000]\n",
      "loss: 1.343611  [ 6400/60000]\n",
      "loss: 1.067509  [12800/60000]\n",
      "loss: 1.060253  [19200/60000]\n",
      "loss: 1.274393  [25600/60000]\n",
      "loss: 0.955891  [32000/60000]\n",
      "loss: 1.332851  [38400/60000]\n",
      "loss: 1.269185  [44800/60000]\n",
      "loss: 1.375910  [51200/60000]\n",
      "loss: 1.235911  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.210671 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.287088  [    0/60000]\n",
      "loss: 1.343176  [ 6400/60000]\n",
      "loss: 1.067024  [12800/60000]\n",
      "loss: 1.059697  [19200/60000]\n",
      "loss: 1.273631  [25600/60000]\n",
      "loss: 0.955389  [32000/60000]\n",
      "loss: 1.332102  [38400/60000]\n",
      "loss: 1.268663  [44800/60000]\n",
      "loss: 1.375168  [51200/60000]\n",
      "loss: 1.235514  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.210310 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.286907  [    0/60000]\n",
      "loss: 1.342723  [ 6400/60000]\n",
      "loss: 1.066671  [12800/60000]\n",
      "loss: 1.059078  [19200/60000]\n",
      "loss: 1.272907  [25600/60000]\n",
      "loss: 0.954627  [32000/60000]\n",
      "loss: 1.331434  [38400/60000]\n",
      "loss: 1.268123  [44800/60000]\n",
      "loss: 1.374309  [51200/60000]\n",
      "loss: 1.235065  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.209930 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.286747  [    0/60000]\n",
      "loss: 1.342302  [ 6400/60000]\n",
      "loss: 1.066170  [12800/60000]\n",
      "loss: 1.058526  [19200/60000]\n",
      "loss: 1.272140  [25600/60000]\n",
      "loss: 0.953824  [32000/60000]\n",
      "loss: 1.330783  [38400/60000]\n",
      "loss: 1.267608  [44800/60000]\n",
      "loss: 1.373630  [51200/60000]\n",
      "loss: 1.234702  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.209574 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.286572  [    0/60000]\n",
      "loss: 1.341867  [ 6400/60000]\n",
      "loss: 1.065622  [12800/60000]\n",
      "loss: 1.057968  [19200/60000]\n",
      "loss: 1.271357  [25600/60000]\n",
      "loss: 0.953145  [32000/60000]\n",
      "loss: 1.329951  [38400/60000]\n",
      "loss: 1.267249  [44800/60000]\n",
      "loss: 1.372800  [51200/60000]\n",
      "loss: 1.234342  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.209216 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.286491  [    0/60000]\n",
      "loss: 1.341413  [ 6400/60000]\n",
      "loss: 1.065201  [12800/60000]\n",
      "loss: 1.057387  [19200/60000]\n",
      "loss: 1.270649  [25600/60000]\n",
      "loss: 0.952566  [32000/60000]\n",
      "loss: 1.329317  [38400/60000]\n",
      "loss: 1.266709  [44800/60000]\n",
      "loss: 1.372150  [51200/60000]\n",
      "loss: 1.233977  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.208861 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.286365  [    0/60000]\n",
      "loss: 1.340932  [ 6400/60000]\n",
      "loss: 1.064813  [12800/60000]\n",
      "loss: 1.056806  [19200/60000]\n",
      "loss: 1.269939  [25600/60000]\n",
      "loss: 0.951900  [32000/60000]\n",
      "loss: 1.328666  [38400/60000]\n",
      "loss: 1.266127  [44800/60000]\n",
      "loss: 1.371494  [51200/60000]\n",
      "loss: 1.233611  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.208502 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.286239  [    0/60000]\n",
      "loss: 1.340541  [ 6400/60000]\n",
      "loss: 1.064405  [12800/60000]\n",
      "loss: 1.056259  [19200/60000]\n",
      "loss: 1.269228  [25600/60000]\n",
      "loss: 0.951186  [32000/60000]\n",
      "loss: 1.328054  [38400/60000]\n",
      "loss: 1.265806  [44800/60000]\n",
      "loss: 1.370827  [51200/60000]\n",
      "loss: 1.233243  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.208147 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.286118  [    0/60000]\n",
      "loss: 1.340137  [ 6400/60000]\n",
      "loss: 1.063968  [12800/60000]\n",
      "loss: 1.055656  [19200/60000]\n",
      "loss: 1.268612  [25600/60000]\n",
      "loss: 0.950501  [32000/60000]\n",
      "loss: 1.327373  [38400/60000]\n",
      "loss: 1.265540  [44800/60000]\n",
      "loss: 1.370165  [51200/60000]\n",
      "loss: 1.232870  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.207790 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.286015  [    0/60000]\n",
      "loss: 1.339750  [ 6400/60000]\n",
      "loss: 1.063643  [12800/60000]\n",
      "loss: 1.055035  [19200/60000]\n",
      "loss: 1.267938  [25600/60000]\n",
      "loss: 0.949845  [32000/60000]\n",
      "loss: 1.326807  [38400/60000]\n",
      "loss: 1.265836  [44800/60000]\n",
      "loss: 1.369396  [51200/60000]\n",
      "loss: 1.232537  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.207437 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.285950  [    0/60000]\n",
      "loss: 1.339369  [ 6400/60000]\n",
      "loss: 1.063219  [12800/60000]\n",
      "loss: 1.054490  [19200/60000]\n",
      "loss: 1.267175  [25600/60000]\n",
      "loss: 0.949282  [32000/60000]\n",
      "loss: 1.326194  [38400/60000]\n",
      "loss: 1.265527  [44800/60000]\n",
      "loss: 1.368695  [51200/60000]\n",
      "loss: 1.232212  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.207101 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.285859  [    0/60000]\n",
      "loss: 1.338936  [ 6400/60000]\n",
      "loss: 1.062864  [12800/60000]\n",
      "loss: 1.053695  [19200/60000]\n",
      "loss: 1.266500  [25600/60000]\n",
      "loss: 0.948593  [32000/60000]\n",
      "loss: 1.325592  [38400/60000]\n",
      "loss: 1.265056  [44800/60000]\n",
      "loss: 1.368131  [51200/60000]\n",
      "loss: 1.231891  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.206758 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.285762  [    0/60000]\n",
      "loss: 1.338517  [ 6400/60000]\n",
      "loss: 1.062405  [12800/60000]\n",
      "loss: 1.053169  [19200/60000]\n",
      "loss: 1.266493  [25600/60000]\n",
      "loss: 0.947937  [32000/60000]\n",
      "loss: 1.324886  [38400/60000]\n",
      "loss: 1.264551  [44800/60000]\n",
      "loss: 1.367551  [51200/60000]\n",
      "loss: 1.231595  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.206420 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.285679  [    0/60000]\n",
      "loss: 1.338121  [ 6400/60000]\n",
      "loss: 1.061923  [12800/60000]\n",
      "loss: 1.052610  [19200/60000]\n",
      "loss: 1.265741  [25600/60000]\n",
      "loss: 0.947314  [32000/60000]\n",
      "loss: 1.324274  [38400/60000]\n",
      "loss: 1.264166  [44800/60000]\n",
      "loss: 1.367059  [51200/60000]\n",
      "loss: 1.231333  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.206079 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.285597  [    0/60000]\n",
      "loss: 1.337698  [ 6400/60000]\n",
      "loss: 1.061469  [12800/60000]\n",
      "loss: 1.052048  [19200/60000]\n",
      "loss: 1.265053  [25600/60000]\n",
      "loss: 0.946540  [32000/60000]\n",
      "loss: 1.323643  [38400/60000]\n",
      "loss: 1.263670  [44800/60000]\n",
      "loss: 1.366454  [51200/60000]\n",
      "loss: 1.230914  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.205737 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 1.285526  [    0/60000]\n",
      "loss: 1.337233  [ 6400/60000]\n",
      "loss: 1.061034  [12800/60000]\n",
      "loss: 1.051491  [19200/60000]\n",
      "loss: 1.264393  [25600/60000]\n",
      "loss: 0.945886  [32000/60000]\n",
      "loss: 1.323030  [38400/60000]\n",
      "loss: 1.263165  [44800/60000]\n",
      "loss: 1.365855  [51200/60000]\n",
      "loss: 1.230569  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.205398 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 1.285383  [    0/60000]\n",
      "loss: 1.336842  [ 6400/60000]\n",
      "loss: 1.060571  [12800/60000]\n",
      "loss: 1.050905  [19200/60000]\n",
      "loss: 1.263730  [25600/60000]\n",
      "loss: 0.945282  [32000/60000]\n",
      "loss: 1.322392  [38400/60000]\n",
      "loss: 1.262714  [44800/60000]\n",
      "loss: 1.365268  [51200/60000]\n",
      "loss: 1.230266  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.205063 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 1.285268  [    0/60000]\n",
      "loss: 1.336452  [ 6400/60000]\n",
      "loss: 1.060156  [12800/60000]\n",
      "loss: 1.050335  [19200/60000]\n",
      "loss: 1.263119  [25600/60000]\n",
      "loss: 0.944702  [32000/60000]\n",
      "loss: 1.321751  [38400/60000]\n",
      "loss: 1.262073  [44800/60000]\n",
      "loss: 1.364660  [51200/60000]\n",
      "loss: 1.229947  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.204732 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 1.285197  [    0/60000]\n",
      "loss: 1.335995  [ 6400/60000]\n",
      "loss: 1.059764  [12800/60000]\n",
      "loss: 1.049760  [19200/60000]\n",
      "loss: 1.262508  [25600/60000]\n",
      "loss: 0.944058  [32000/60000]\n",
      "loss: 1.321102  [38400/60000]\n",
      "loss: 1.261789  [44800/60000]\n",
      "loss: 1.364065  [51200/60000]\n",
      "loss: 1.229629  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.204400 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 1.285154  [    0/60000]\n",
      "loss: 1.335628  [ 6400/60000]\n",
      "loss: 1.059350  [12800/60000]\n",
      "loss: 1.049272  [19200/60000]\n",
      "loss: 1.261866  [25600/60000]\n",
      "loss: 0.943365  [32000/60000]\n",
      "loss: 1.320513  [38400/60000]\n",
      "loss: 1.261378  [44800/60000]\n",
      "loss: 1.363403  [51200/60000]\n",
      "loss: 1.229341  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.204108 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 1.285043  [    0/60000]\n",
      "loss: 1.335081  [ 6400/60000]\n",
      "loss: 1.059006  [12800/60000]\n",
      "loss: 1.048694  [19200/60000]\n",
      "loss: 1.261240  [25600/60000]\n",
      "loss: 0.942750  [32000/60000]\n",
      "loss: 1.320014  [38400/60000]\n",
      "loss: 1.260857  [44800/60000]\n",
      "loss: 1.362856  [51200/60000]\n",
      "loss: 1.229069  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.203782 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 1.284986  [    0/60000]\n",
      "loss: 1.334689  [ 6400/60000]\n",
      "loss: 1.058680  [12800/60000]\n",
      "loss: 1.048124  [19200/60000]\n",
      "loss: 1.260716  [25600/60000]\n",
      "loss: 0.941689  [32000/60000]\n",
      "loss: 1.319443  [38400/60000]\n",
      "loss: 1.260428  [44800/60000]\n",
      "loss: 1.362250  [51200/60000]\n",
      "loss: 1.228775  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.203462 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 1.284907  [    0/60000]\n",
      "loss: 1.334279  [ 6400/60000]\n",
      "loss: 1.058335  [12800/60000]\n",
      "loss: 1.047612  [19200/60000]\n",
      "loss: 1.260087  [25600/60000]\n",
      "loss: 0.940996  [32000/60000]\n",
      "loss: 1.318831  [38400/60000]\n",
      "loss: 1.259962  [44800/60000]\n",
      "loss: 1.361682  [51200/60000]\n",
      "loss: 1.228428  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.203139 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 1.284831  [    0/60000]\n",
      "loss: 1.333918  [ 6400/60000]\n",
      "loss: 1.057981  [12800/60000]\n",
      "loss: 1.047118  [19200/60000]\n",
      "loss: 1.259553  [25600/60000]\n",
      "loss: 0.940395  [32000/60000]\n",
      "loss: 1.318251  [38400/60000]\n",
      "loss: 1.259527  [44800/60000]\n",
      "loss: 1.361207  [51200/60000]\n",
      "loss: 1.228108  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.202822 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 1.284754  [    0/60000]\n",
      "loss: 1.333537  [ 6400/60000]\n",
      "loss: 1.057629  [12800/60000]\n",
      "loss: 1.046589  [19200/60000]\n",
      "loss: 1.258974  [25600/60000]\n",
      "loss: 0.939898  [32000/60000]\n",
      "loss: 1.317719  [38400/60000]\n",
      "loss: 1.259076  [44800/60000]\n",
      "loss: 1.360694  [51200/60000]\n",
      "loss: 1.227839  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.202507 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 1.284689  [    0/60000]\n",
      "loss: 1.333150  [ 6400/60000]\n",
      "loss: 1.057308  [12800/60000]\n",
      "loss: 1.046023  [19200/60000]\n",
      "loss: 1.258461  [25600/60000]\n",
      "loss: 0.939398  [32000/60000]\n",
      "loss: 1.317060  [38400/60000]\n",
      "loss: 1.258584  [44800/60000]\n",
      "loss: 1.360151  [51200/60000]\n",
      "loss: 1.227539  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.202191 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 1.284614  [    0/60000]\n",
      "loss: 1.332629  [ 6400/60000]\n",
      "loss: 1.056959  [12800/60000]\n",
      "loss: 1.045480  [19200/60000]\n",
      "loss: 1.257819  [25600/60000]\n",
      "loss: 0.938845  [32000/60000]\n",
      "loss: 1.316477  [38400/60000]\n",
      "loss: 1.258202  [44800/60000]\n",
      "loss: 1.359566  [51200/60000]\n",
      "loss: 1.227239  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.201868 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 1.284539  [    0/60000]\n",
      "loss: 1.332236  [ 6400/60000]\n",
      "loss: 1.056602  [12800/60000]\n",
      "loss: 1.044886  [19200/60000]\n",
      "loss: 1.257150  [25600/60000]\n",
      "loss: 0.938195  [32000/60000]\n",
      "loss: 1.315938  [38400/60000]\n",
      "loss: 1.257689  [44800/60000]\n",
      "loss: 1.359000  [51200/60000]\n",
      "loss: 1.226986  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.201553 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 1.284465  [    0/60000]\n",
      "loss: 1.331830  [ 6400/60000]\n",
      "loss: 1.056264  [12800/60000]\n",
      "loss: 1.044345  [19200/60000]\n",
      "loss: 1.256574  [25600/60000]\n",
      "loss: 0.937615  [32000/60000]\n",
      "loss: 1.315326  [38400/60000]\n",
      "loss: 1.257352  [44800/60000]\n",
      "loss: 1.358432  [51200/60000]\n",
      "loss: 1.226488  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.201252 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 1.284544  [    0/60000]\n",
      "loss: 1.331388  [ 6400/60000]\n",
      "loss: 1.055993  [12800/60000]\n",
      "loss: 1.043769  [19200/60000]\n",
      "loss: 1.255914  [25600/60000]\n",
      "loss: 0.937060  [32000/60000]\n",
      "loss: 1.314767  [38400/60000]\n",
      "loss: 1.256869  [44800/60000]\n",
      "loss: 1.357932  [51200/60000]\n",
      "loss: 1.226307  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.200945 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 1.284467  [    0/60000]\n",
      "loss: 1.330998  [ 6400/60000]\n",
      "loss: 1.055646  [12800/60000]\n",
      "loss: 1.043243  [19200/60000]\n",
      "loss: 1.255314  [25600/60000]\n",
      "loss: 0.936363  [32000/60000]\n",
      "loss: 1.314202  [38400/60000]\n",
      "loss: 1.256481  [44800/60000]\n",
      "loss: 1.357399  [51200/60000]\n",
      "loss: 1.226065  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.200634 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 1.284448  [    0/60000]\n",
      "loss: 1.330554  [ 6400/60000]\n",
      "loss: 1.055314  [12800/60000]\n",
      "loss: 1.042749  [19200/60000]\n",
      "loss: 1.254676  [25600/60000]\n",
      "loss: 0.935741  [32000/60000]\n",
      "loss: 1.313638  [38400/60000]\n",
      "loss: 1.256036  [44800/60000]\n",
      "loss: 1.356831  [51200/60000]\n",
      "loss: 1.225814  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.200328 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 1.284420  [    0/60000]\n",
      "loss: 1.330146  [ 6400/60000]\n",
      "loss: 1.054971  [12800/60000]\n",
      "loss: 1.042219  [19200/60000]\n",
      "loss: 1.254063  [25600/60000]\n",
      "loss: 0.935173  [32000/60000]\n",
      "loss: 1.313142  [38400/60000]\n",
      "loss: 1.255661  [44800/60000]\n",
      "loss: 1.356231  [51200/60000]\n",
      "loss: 1.225517  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.200018 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 1.284357  [    0/60000]\n",
      "loss: 1.329757  [ 6400/60000]\n",
      "loss: 1.054592  [12800/60000]\n",
      "loss: 1.041677  [19200/60000]\n",
      "loss: 1.253417  [25600/60000]\n",
      "loss: 0.934647  [32000/60000]\n",
      "loss: 1.312596  [38400/60000]\n",
      "loss: 1.255265  [44800/60000]\n",
      "loss: 1.355689  [51200/60000]\n",
      "loss: 1.225222  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.199706 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 1.284254  [    0/60000]\n",
      "loss: 1.329395  [ 6400/60000]\n",
      "loss: 1.054256  [12800/60000]\n",
      "loss: 1.041166  [19200/60000]\n",
      "loss: 1.252781  [25600/60000]\n",
      "loss: 0.934119  [32000/60000]\n",
      "loss: 1.312131  [38400/60000]\n",
      "loss: 1.254788  [44800/60000]\n",
      "loss: 1.355091  [51200/60000]\n",
      "loss: 1.224917  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.199396 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 1.284100  [    0/60000]\n",
      "loss: 1.329085  [ 6400/60000]\n",
      "loss: 1.053770  [12800/60000]\n",
      "loss: 1.040672  [19200/60000]\n",
      "loss: 1.252148  [25600/60000]\n",
      "loss: 0.933098  [32000/60000]\n",
      "loss: 1.311763  [38400/60000]\n",
      "loss: 1.254375  [44800/60000]\n",
      "loss: 1.354458  [51200/60000]\n",
      "loss: 1.224636  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.199096 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 1.284013  [    0/60000]\n",
      "loss: 1.328730  [ 6400/60000]\n",
      "loss: 1.053428  [12800/60000]\n",
      "loss: 1.040160  [19200/60000]\n",
      "loss: 1.251731  [25600/60000]\n",
      "loss: 0.932533  [32000/60000]\n",
      "loss: 1.311285  [38400/60000]\n",
      "loss: 1.253937  [44800/60000]\n",
      "loss: 1.353942  [51200/60000]\n",
      "loss: 1.224347  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 1.198785 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 1.283935  [    0/60000]\n",
      "loss: 1.328422  [ 6400/60000]\n",
      "loss: 1.053104  [12800/60000]\n",
      "loss: 1.039619  [19200/60000]\n",
      "loss: 1.251098  [25600/60000]\n",
      "loss: 0.932004  [32000/60000]\n",
      "loss: 1.310734  [38400/60000]\n",
      "loss: 1.253472  [44800/60000]\n",
      "loss: 1.353356  [51200/60000]\n",
      "loss: 1.224038  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.198487 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 1.283851  [    0/60000]\n",
      "loss: 1.328097  [ 6400/60000]\n",
      "loss: 1.052743  [12800/60000]\n",
      "loss: 1.039095  [19200/60000]\n",
      "loss: 1.250503  [25600/60000]\n",
      "loss: 0.931399  [32000/60000]\n",
      "loss: 1.310189  [38400/60000]\n",
      "loss: 1.253192  [44800/60000]\n",
      "loss: 1.352844  [51200/60000]\n",
      "loss: 1.223755  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.198175 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 1.283753  [    0/60000]\n",
      "loss: 1.327773  [ 6400/60000]\n",
      "loss: 1.052428  [12800/60000]\n",
      "loss: 1.038580  [19200/60000]\n",
      "loss: 1.249926  [25600/60000]\n",
      "loss: 0.930537  [32000/60000]\n",
      "loss: 1.309705  [38400/60000]\n",
      "loss: 1.252857  [44800/60000]\n",
      "loss: 1.352293  [51200/60000]\n",
      "loss: 1.223465  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.197870 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 1.283683  [    0/60000]\n",
      "loss: 1.327456  [ 6400/60000]\n",
      "loss: 1.052053  [12800/60000]\n",
      "loss: 1.037998  [19200/60000]\n",
      "loss: 1.249350  [25600/60000]\n",
      "loss: 0.930483  [32000/60000]\n",
      "loss: 1.309254  [38400/60000]\n",
      "loss: 1.252540  [44800/60000]\n",
      "loss: 1.351774  [51200/60000]\n",
      "loss: 1.223237  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.197573 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 1.283646  [    0/60000]\n",
      "loss: 1.327163  [ 6400/60000]\n",
      "loss: 1.051744  [12800/60000]\n",
      "loss: 1.037520  [19200/60000]\n",
      "loss: 1.248798  [25600/60000]\n",
      "loss: 0.929960  [32000/60000]\n",
      "loss: 1.308784  [38400/60000]\n",
      "loss: 1.252109  [44800/60000]\n",
      "loss: 1.351265  [51200/60000]\n",
      "loss: 1.222977  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.197274 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 1.283577  [    0/60000]\n",
      "loss: 1.326827  [ 6400/60000]\n",
      "loss: 1.051355  [12800/60000]\n",
      "loss: 1.037023  [19200/60000]\n",
      "loss: 1.248186  [25600/60000]\n",
      "loss: 0.929499  [32000/60000]\n",
      "loss: 1.308310  [38400/60000]\n",
      "loss: 1.251710  [44800/60000]\n",
      "loss: 1.350759  [51200/60000]\n",
      "loss: 1.222768  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.196974 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 1.283486  [    0/60000]\n",
      "loss: 1.326462  [ 6400/60000]\n",
      "loss: 1.051038  [12800/60000]\n",
      "loss: 1.036474  [19200/60000]\n",
      "loss: 1.247597  [25600/60000]\n",
      "loss: 0.928985  [32000/60000]\n",
      "loss: 1.307823  [38400/60000]\n",
      "loss: 1.251344  [44800/60000]\n",
      "loss: 1.350270  [51200/60000]\n",
      "loss: 1.222558  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.196680 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 1.283410  [    0/60000]\n",
      "loss: 1.326117  [ 6400/60000]\n",
      "loss: 1.050736  [12800/60000]\n",
      "loss: 1.035975  [19200/60000]\n",
      "loss: 1.246994  [25600/60000]\n",
      "loss: 0.928531  [32000/60000]\n",
      "loss: 1.307390  [38400/60000]\n",
      "loss: 1.250998  [44800/60000]\n",
      "loss: 1.349755  [51200/60000]\n",
      "loss: 1.222316  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.196385 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 1.283374  [    0/60000]\n",
      "loss: 1.325782  [ 6400/60000]\n",
      "loss: 1.050470  [12800/60000]\n",
      "loss: 1.035493  [19200/60000]\n",
      "loss: 1.246415  [25600/60000]\n",
      "loss: 0.926828  [32000/60000]\n",
      "loss: 1.306861  [38400/60000]\n",
      "loss: 1.250648  [44800/60000]\n",
      "loss: 1.349213  [51200/60000]\n",
      "loss: 1.222025  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.196078 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 1.283328  [    0/60000]\n",
      "loss: 1.325501  [ 6400/60000]\n",
      "loss: 1.050195  [12800/60000]\n",
      "loss: 1.034991  [19200/60000]\n",
      "loss: 1.245809  [25600/60000]\n",
      "loss: 0.926298  [32000/60000]\n",
      "loss: 1.306417  [38400/60000]\n",
      "loss: 1.250308  [44800/60000]\n",
      "loss: 1.348624  [51200/60000]\n",
      "loss: 1.221786  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.195784 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 1.283271  [    0/60000]\n",
      "loss: 1.325147  [ 6400/60000]\n",
      "loss: 1.049941  [12800/60000]\n",
      "loss: 1.034449  [19200/60000]\n",
      "loss: 1.245224  [25600/60000]\n",
      "loss: 0.925825  [32000/60000]\n",
      "loss: 1.305941  [38400/60000]\n",
      "loss: 1.249919  [44800/60000]\n",
      "loss: 1.348125  [51200/60000]\n",
      "loss: 1.221495  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.195490 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 1.283214  [    0/60000]\n",
      "loss: 1.324810  [ 6400/60000]\n",
      "loss: 1.049682  [12800/60000]\n",
      "loss: 1.033954  [19200/60000]\n",
      "loss: 1.244637  [25600/60000]\n",
      "loss: 0.925280  [32000/60000]\n",
      "loss: 1.305522  [38400/60000]\n",
      "loss: 1.249516  [44800/60000]\n",
      "loss: 1.347599  [51200/60000]\n",
      "loss: 1.221200  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.195198 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 1.283170  [    0/60000]\n",
      "loss: 1.324513  [ 6400/60000]\n",
      "loss: 1.049427  [12800/60000]\n",
      "loss: 1.033397  [19200/60000]\n",
      "loss: 1.244084  [25600/60000]\n",
      "loss: 0.924722  [32000/60000]\n",
      "loss: 1.305061  [38400/60000]\n",
      "loss: 1.249175  [44800/60000]\n",
      "loss: 1.347123  [51200/60000]\n",
      "loss: 1.220921  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.194911 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 1.283191  [    0/60000]\n",
      "loss: 1.324198  [ 6400/60000]\n",
      "loss: 1.049134  [12800/60000]\n",
      "loss: 1.032887  [19200/60000]\n",
      "loss: 1.243530  [25600/60000]\n",
      "loss: 0.924212  [32000/60000]\n",
      "loss: 1.304622  [38400/60000]\n",
      "loss: 1.248789  [44800/60000]\n",
      "loss: 1.346630  [51200/60000]\n",
      "loss: 1.220687  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.194620 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 1.283159  [    0/60000]\n",
      "loss: 1.323862  [ 6400/60000]\n",
      "loss: 1.048881  [12800/60000]\n",
      "loss: 1.032385  [19200/60000]\n",
      "loss: 1.243025  [25600/60000]\n",
      "loss: 0.923810  [32000/60000]\n",
      "loss: 1.304235  [38400/60000]\n",
      "loss: 1.248422  [44800/60000]\n",
      "loss: 1.346157  [51200/60000]\n",
      "loss: 1.220481  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.194332 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 1.283128  [    0/60000]\n",
      "loss: 1.323552  [ 6400/60000]\n",
      "loss: 1.048657  [12800/60000]\n",
      "loss: 1.031906  [19200/60000]\n",
      "loss: 1.242470  [25600/60000]\n",
      "loss: 0.923331  [32000/60000]\n",
      "loss: 1.303862  [38400/60000]\n",
      "loss: 1.248009  [44800/60000]\n",
      "loss: 1.345601  [51200/60000]\n",
      "loss: 1.220255  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.194046 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 1.283062  [    0/60000]\n",
      "loss: 1.323223  [ 6400/60000]\n",
      "loss: 1.048411  [12800/60000]\n",
      "loss: 1.031388  [19200/60000]\n",
      "loss: 1.241913  [25600/60000]\n",
      "loss: 0.922814  [32000/60000]\n",
      "loss: 1.303276  [38400/60000]\n",
      "loss: 1.247658  [44800/60000]\n",
      "loss: 1.345038  [51200/60000]\n",
      "loss: 1.220056  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.193758 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 1.282985  [    0/60000]\n",
      "loss: 1.322926  [ 6400/60000]\n",
      "loss: 1.048096  [12800/60000]\n",
      "loss: 1.030840  [19200/60000]\n",
      "loss: 1.241340  [25600/60000]\n",
      "loss: 0.922358  [32000/60000]\n",
      "loss: 1.302822  [38400/60000]\n",
      "loss: 1.247297  [44800/60000]\n",
      "loss: 1.344518  [51200/60000]\n",
      "loss: 1.219869  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.193472 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 1.282917  [    0/60000]\n",
      "loss: 1.322568  [ 6400/60000]\n",
      "loss: 1.047844  [12800/60000]\n",
      "loss: 1.030350  [19200/60000]\n",
      "loss: 1.241152  [25600/60000]\n",
      "loss: 0.921865  [32000/60000]\n",
      "loss: 1.302319  [38400/60000]\n",
      "loss: 1.246879  [44800/60000]\n",
      "loss: 1.344025  [51200/60000]\n",
      "loss: 1.219691  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.193194 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 1.282856  [    0/60000]\n",
      "loss: 1.322281  [ 6400/60000]\n",
      "loss: 1.047577  [12800/60000]\n",
      "loss: 1.029884  [19200/60000]\n",
      "loss: 1.240597  [25600/60000]\n",
      "loss: 0.921431  [32000/60000]\n",
      "loss: 1.301771  [38400/60000]\n",
      "loss: 1.246455  [44800/60000]\n",
      "loss: 1.343527  [51200/60000]\n",
      "loss: 1.219488  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.192605 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 1.283012  [    0/60000]\n",
      "loss: 1.321964  [ 6400/60000]\n",
      "loss: 1.047357  [12800/60000]\n",
      "loss: 1.029399  [19200/60000]\n",
      "loss: 1.239967  [25600/60000]\n",
      "loss: 0.921200  [32000/60000]\n",
      "loss: 1.301202  [38400/60000]\n",
      "loss: 1.245981  [44800/60000]\n",
      "loss: 1.342996  [51200/60000]\n",
      "loss: 1.219329  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.192328 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 1.282959  [    0/60000]\n",
      "loss: 1.321688  [ 6400/60000]\n",
      "loss: 1.047120  [12800/60000]\n",
      "loss: 1.028832  [19200/60000]\n",
      "loss: 1.239389  [25600/60000]\n",
      "loss: 0.920794  [32000/60000]\n",
      "loss: 1.300686  [38400/60000]\n",
      "loss: 1.245510  [44800/60000]\n",
      "loss: 1.342508  [51200/60000]\n",
      "loss: 1.219164  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.192036 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 1.282918  [    0/60000]\n",
      "loss: 1.321395  [ 6400/60000]\n",
      "loss: 1.046855  [12800/60000]\n",
      "loss: 1.028287  [19200/60000]\n",
      "loss: 1.238801  [25600/60000]\n",
      "loss: 0.920383  [32000/60000]\n",
      "loss: 1.300205  [38400/60000]\n",
      "loss: 1.245037  [44800/60000]\n",
      "loss: 1.342016  [51200/60000]\n",
      "loss: 1.219024  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.191762 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 1.282843  [    0/60000]\n",
      "loss: 1.321087  [ 6400/60000]\n",
      "loss: 1.046582  [12800/60000]\n",
      "loss: 1.027782  [19200/60000]\n",
      "loss: 1.238244  [25600/60000]\n",
      "loss: 0.920047  [32000/60000]\n",
      "loss: 1.299776  [38400/60000]\n",
      "loss: 1.244645  [44800/60000]\n",
      "loss: 1.341547  [51200/60000]\n",
      "loss: 1.218882  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.191492 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 1.282774  [    0/60000]\n",
      "loss: 1.320773  [ 6400/60000]\n",
      "loss: 1.046323  [12800/60000]\n",
      "loss: 1.027339  [19200/60000]\n",
      "loss: 1.237697  [25600/60000]\n",
      "loss: 0.919644  [32000/60000]\n",
      "loss: 1.299361  [38400/60000]\n",
      "loss: 1.244352  [44800/60000]\n",
      "loss: 1.341094  [51200/60000]\n",
      "loss: 1.218741  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.191234 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 1.282686  [    0/60000]\n",
      "loss: 1.320481  [ 6400/60000]\n",
      "loss: 1.046083  [12800/60000]\n",
      "loss: 1.026842  [19200/60000]\n",
      "loss: 1.237138  [25600/60000]\n",
      "loss: 0.919215  [32000/60000]\n",
      "loss: 1.298904  [38400/60000]\n",
      "loss: 1.243959  [44800/60000]\n",
      "loss: 1.340550  [51200/60000]\n",
      "loss: 1.218582  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.190961 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 1.282647  [    0/60000]\n",
      "loss: 1.320147  [ 6400/60000]\n",
      "loss: 1.045832  [12800/60000]\n",
      "loss: 1.026394  [19200/60000]\n",
      "loss: 1.236600  [25600/60000]\n",
      "loss: 0.918816  [32000/60000]\n",
      "loss: 1.298443  [38400/60000]\n",
      "loss: 1.243615  [44800/60000]\n",
      "loss: 1.340017  [51200/60000]\n",
      "loss: 1.218419  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.190699 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 1.282581  [    0/60000]\n",
      "loss: 1.319747  [ 6400/60000]\n",
      "loss: 1.045582  [12800/60000]\n",
      "loss: 1.025928  [19200/60000]\n",
      "loss: 1.236018  [25600/60000]\n",
      "loss: 0.918397  [32000/60000]\n",
      "loss: 1.298059  [38400/60000]\n",
      "loss: 1.243218  [44800/60000]\n",
      "loss: 1.339565  [51200/60000]\n",
      "loss: 1.218242  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.190431 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 1.282519  [    0/60000]\n",
      "loss: 1.319395  [ 6400/60000]\n",
      "loss: 1.045405  [12800/60000]\n",
      "loss: 1.025456  [19200/60000]\n",
      "loss: 1.235540  [25600/60000]\n",
      "loss: 0.917994  [32000/60000]\n",
      "loss: 1.297691  [38400/60000]\n",
      "loss: 1.242861  [44800/60000]\n",
      "loss: 1.339097  [51200/60000]\n",
      "loss: 1.218013  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.190168 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 1.282500  [    0/60000]\n",
      "loss: 1.319043  [ 6400/60000]\n",
      "loss: 1.045214  [12800/60000]\n",
      "loss: 1.025053  [19200/60000]\n",
      "loss: 1.235041  [25600/60000]\n",
      "loss: 0.917556  [32000/60000]\n",
      "loss: 1.297279  [38400/60000]\n",
      "loss: 1.242436  [44800/60000]\n",
      "loss: 1.338581  [51200/60000]\n",
      "loss: 1.217811  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.189908 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 1.282464  [    0/60000]\n",
      "loss: 1.318727  [ 6400/60000]\n",
      "loss: 1.044999  [12800/60000]\n",
      "loss: 1.024618  [19200/60000]\n",
      "loss: 1.234500  [25600/60000]\n",
      "loss: 0.917172  [32000/60000]\n",
      "loss: 1.296973  [38400/60000]\n",
      "loss: 1.242037  [44800/60000]\n",
      "loss: 1.338081  [51200/60000]\n",
      "loss: 1.217551  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.189640 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 1.282423  [    0/60000]\n",
      "loss: 1.318390  [ 6400/60000]\n",
      "loss: 1.044766  [12800/60000]\n",
      "loss: 1.024155  [19200/60000]\n",
      "loss: 1.234001  [25600/60000]\n",
      "loss: 0.916744  [32000/60000]\n",
      "loss: 1.296569  [38400/60000]\n",
      "loss: 1.241684  [44800/60000]\n",
      "loss: 1.337611  [51200/60000]\n",
      "loss: 1.217272  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.189379 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 1.282361  [    0/60000]\n",
      "loss: 1.318078  [ 6400/60000]\n",
      "loss: 1.044568  [12800/60000]\n",
      "loss: 1.023705  [19200/60000]\n",
      "loss: 1.233503  [25600/60000]\n",
      "loss: 0.916369  [32000/60000]\n",
      "loss: 1.296203  [38400/60000]\n",
      "loss: 1.241205  [44800/60000]\n",
      "loss: 1.337162  [51200/60000]\n",
      "loss: 1.217079  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.189118 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 1.282279  [    0/60000]\n",
      "loss: 1.317745  [ 6400/60000]\n",
      "loss: 1.044387  [12800/60000]\n",
      "loss: 1.022712  [19200/60000]\n",
      "loss: 1.233033  [25600/60000]\n",
      "loss: 0.915898  [32000/60000]\n",
      "loss: 1.295825  [38400/60000]\n",
      "loss: 1.240797  [44800/60000]\n",
      "loss: 1.336690  [51200/60000]\n",
      "loss: 1.216877  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.188853 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 1.282247  [    0/60000]\n",
      "loss: 1.317348  [ 6400/60000]\n",
      "loss: 1.044212  [12800/60000]\n",
      "loss: 1.022220  [19200/60000]\n",
      "loss: 1.232534  [25600/60000]\n",
      "loss: 0.915335  [32000/60000]\n",
      "loss: 1.295582  [38400/60000]\n",
      "loss: 1.240376  [44800/60000]\n",
      "loss: 1.336149  [51200/60000]\n",
      "loss: 1.216664  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.188594 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 1.282189  [    0/60000]\n",
      "loss: 1.317050  [ 6400/60000]\n",
      "loss: 1.044014  [12800/60000]\n",
      "loss: 1.021722  [19200/60000]\n",
      "loss: 1.232058  [25600/60000]\n",
      "loss: 0.914911  [32000/60000]\n",
      "loss: 1.295066  [38400/60000]\n",
      "loss: 1.240005  [44800/60000]\n",
      "loss: 1.335665  [51200/60000]\n",
      "loss: 1.216472  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.188333 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 1.282139  [    0/60000]\n",
      "loss: 1.316489  [ 6400/60000]\n",
      "loss: 1.043870  [12800/60000]\n",
      "loss: 1.021277  [19200/60000]\n",
      "loss: 1.231621  [25600/60000]\n",
      "loss: 0.914559  [32000/60000]\n",
      "loss: 1.294717  [38400/60000]\n",
      "loss: 1.239570  [44800/60000]\n",
      "loss: 1.335200  [51200/60000]\n",
      "loss: 1.216303  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.188068 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 1.282050  [    0/60000]\n",
      "loss: 1.316135  [ 6400/60000]\n",
      "loss: 1.043737  [12800/60000]\n",
      "loss: 1.020813  [19200/60000]\n",
      "loss: 1.231164  [25600/60000]\n",
      "loss: 0.914095  [32000/60000]\n",
      "loss: 1.294325  [38400/60000]\n",
      "loss: 1.239077  [44800/60000]\n",
      "loss: 1.334725  [51200/60000]\n",
      "loss: 1.216097  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.187811 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 1.281989  [    0/60000]\n",
      "loss: 1.315797  [ 6400/60000]\n",
      "loss: 1.043641  [12800/60000]\n",
      "loss: 1.020391  [19200/60000]\n",
      "loss: 1.230749  [25600/60000]\n",
      "loss: 0.913765  [32000/60000]\n",
      "loss: 1.294019  [38400/60000]\n",
      "loss: 1.238604  [44800/60000]\n",
      "loss: 1.334212  [51200/60000]\n",
      "loss: 1.215925  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.187559 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 1.281929  [    0/60000]\n",
      "loss: 1.315418  [ 6400/60000]\n",
      "loss: 1.043522  [12800/60000]\n",
      "loss: 1.019974  [19200/60000]\n",
      "loss: 1.230385  [25600/60000]\n",
      "loss: 0.913304  [32000/60000]\n",
      "loss: 1.293738  [38400/60000]\n",
      "loss: 1.237999  [44800/60000]\n",
      "loss: 1.333747  [51200/60000]\n",
      "loss: 1.215767  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.187311 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 1.281896  [    0/60000]\n",
      "loss: 1.315071  [ 6400/60000]\n",
      "loss: 1.043360  [12800/60000]\n",
      "loss: 1.019564  [19200/60000]\n",
      "loss: 1.229959  [25600/60000]\n",
      "loss: 0.912960  [32000/60000]\n",
      "loss: 1.293442  [38400/60000]\n",
      "loss: 1.237630  [44800/60000]\n",
      "loss: 1.333305  [51200/60000]\n",
      "loss: 1.215624  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.187058 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 1.281840  [    0/60000]\n",
      "loss: 1.314681  [ 6400/60000]\n",
      "loss: 1.043220  [12800/60000]\n",
      "loss: 1.019158  [19200/60000]\n",
      "loss: 1.229543  [25600/60000]\n",
      "loss: 0.912580  [32000/60000]\n",
      "loss: 1.293009  [38400/60000]\n",
      "loss: 1.237287  [44800/60000]\n",
      "loss: 1.332858  [51200/60000]\n",
      "loss: 1.215441  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.186807 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 1.281757  [    0/60000]\n",
      "loss: 1.314329  [ 6400/60000]\n",
      "loss: 1.043032  [12800/60000]\n",
      "loss: 1.018788  [19200/60000]\n",
      "loss: 1.229105  [25600/60000]\n",
      "loss: 0.912256  [32000/60000]\n",
      "loss: 1.292638  [38400/60000]\n",
      "loss: 1.236911  [44800/60000]\n",
      "loss: 1.332397  [51200/60000]\n",
      "loss: 1.215254  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.186554 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 1.281633  [    0/60000]\n",
      "loss: 1.314014  [ 6400/60000]\n",
      "loss: 1.042772  [12800/60000]\n",
      "loss: 1.018384  [19200/60000]\n",
      "loss: 1.228806  [25600/60000]\n",
      "loss: 0.911745  [32000/60000]\n",
      "loss: 1.292225  [38400/60000]\n",
      "loss: 1.236554  [44800/60000]\n",
      "loss: 1.331888  [51200/60000]\n",
      "loss: 1.215063  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.186322 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 1.281571  [    0/60000]\n",
      "loss: 1.313676  [ 6400/60000]\n",
      "loss: 1.042583  [12800/60000]\n",
      "loss: 1.018008  [19200/60000]\n",
      "loss: 1.228402  [25600/60000]\n",
      "loss: 0.911490  [32000/60000]\n",
      "loss: 1.291845  [38400/60000]\n",
      "loss: 1.236157  [44800/60000]\n",
      "loss: 1.331376  [51200/60000]\n",
      "loss: 1.214867  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.186077 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 1.281497  [    0/60000]\n",
      "loss: 1.313262  [ 6400/60000]\n",
      "loss: 1.042447  [12800/60000]\n",
      "loss: 1.017573  [19200/60000]\n",
      "loss: 1.227717  [25600/60000]\n",
      "loss: 0.911120  [32000/60000]\n",
      "loss: 1.291455  [38400/60000]\n",
      "loss: 1.235707  [44800/60000]\n",
      "loss: 1.330942  [51200/60000]\n",
      "loss: 1.214756  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.185817 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 1.281452  [    0/60000]\n",
      "loss: 1.312930  [ 6400/60000]\n",
      "loss: 1.042329  [12800/60000]\n",
      "loss: 1.017183  [19200/60000]\n",
      "loss: 1.227268  [25600/60000]\n",
      "loss: 0.910823  [32000/60000]\n",
      "loss: 1.291079  [38400/60000]\n",
      "loss: 1.235316  [44800/60000]\n",
      "loss: 1.330516  [51200/60000]\n",
      "loss: 1.214587  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.185565 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 1.281384  [    0/60000]\n",
      "loss: 1.312509  [ 6400/60000]\n",
      "loss: 1.042198  [12800/60000]\n",
      "loss: 1.016852  [19200/60000]\n",
      "loss: 1.226846  [25600/60000]\n",
      "loss: 0.910537  [32000/60000]\n",
      "loss: 1.290708  [38400/60000]\n",
      "loss: 1.235044  [44800/60000]\n",
      "loss: 1.330108  [51200/60000]\n",
      "loss: 1.214427  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.185313 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 1.281392  [    0/60000]\n",
      "loss: 1.312161  [ 6400/60000]\n",
      "loss: 1.042102  [12800/60000]\n",
      "loss: 1.016573  [19200/60000]\n",
      "loss: 1.226480  [25600/60000]\n",
      "loss: 0.910229  [32000/60000]\n",
      "loss: 1.290382  [38400/60000]\n",
      "loss: 1.234663  [44800/60000]\n",
      "loss: 1.329726  [51200/60000]\n",
      "loss: 1.214294  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.185091 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 1.281201  [    0/60000]\n",
      "loss: 1.311832  [ 6400/60000]\n",
      "loss: 1.041942  [12800/60000]\n",
      "loss: 1.016219  [19200/60000]\n",
      "loss: 1.226082  [25600/60000]\n",
      "loss: 0.909853  [32000/60000]\n",
      "loss: 1.290037  [38400/60000]\n",
      "loss: 1.234210  [44800/60000]\n",
      "loss: 1.329249  [51200/60000]\n",
      "loss: 1.214124  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.184847 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 1.281178  [    0/60000]\n",
      "loss: 1.311515  [ 6400/60000]\n",
      "loss: 1.041805  [12800/60000]\n",
      "loss: 1.015868  [19200/60000]\n",
      "loss: 1.225657  [25600/60000]\n",
      "loss: 0.909575  [32000/60000]\n",
      "loss: 1.289734  [38400/60000]\n",
      "loss: 1.233815  [44800/60000]\n",
      "loss: 1.328800  [51200/60000]\n",
      "loss: 1.213944  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.184598 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 1.281090  [    0/60000]\n",
      "loss: 1.311223  [ 6400/60000]\n",
      "loss: 1.041633  [12800/60000]\n",
      "loss: 1.015561  [19200/60000]\n",
      "loss: 1.225322  [25600/60000]\n",
      "loss: 0.909327  [32000/60000]\n",
      "loss: 1.289298  [38400/60000]\n",
      "loss: 1.233345  [44800/60000]\n",
      "loss: 1.328321  [51200/60000]\n",
      "loss: 1.213723  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.184348 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 1.281005  [    0/60000]\n",
      "loss: 1.310936  [ 6400/60000]\n",
      "loss: 1.041469  [12800/60000]\n",
      "loss: 1.015196  [19200/60000]\n",
      "loss: 1.225025  [25600/60000]\n",
      "loss: 0.909073  [32000/60000]\n",
      "loss: 1.288989  [38400/60000]\n",
      "loss: 1.232926  [44800/60000]\n",
      "loss: 1.327830  [51200/60000]\n",
      "loss: 1.213583  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.184113 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 1.280894  [    0/60000]\n",
      "loss: 1.310671  [ 6400/60000]\n",
      "loss: 1.041338  [12800/60000]\n",
      "loss: 1.014786  [19200/60000]\n",
      "loss: 1.224613  [25600/60000]\n",
      "loss: 0.908784  [32000/60000]\n",
      "loss: 1.288637  [38400/60000]\n",
      "loss: 1.232563  [44800/60000]\n",
      "loss: 1.327381  [51200/60000]\n",
      "loss: 1.213352  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.183862 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 1.280784  [    0/60000]\n",
      "loss: 1.310394  [ 6400/60000]\n",
      "loss: 1.041202  [12800/60000]\n",
      "loss: 1.014446  [19200/60000]\n",
      "loss: 1.224185  [25600/60000]\n",
      "loss: 0.908460  [32000/60000]\n",
      "loss: 1.288241  [38400/60000]\n",
      "loss: 1.232283  [44800/60000]\n",
      "loss: 1.326919  [51200/60000]\n",
      "loss: 1.213182  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.183615 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 1.280793  [    0/60000]\n",
      "loss: 1.310054  [ 6400/60000]\n",
      "loss: 1.041187  [12800/60000]\n",
      "loss: 1.014082  [19200/60000]\n",
      "loss: 1.223849  [25600/60000]\n",
      "loss: 0.908259  [32000/60000]\n",
      "loss: 1.287920  [38400/60000]\n",
      "loss: 1.231847  [44800/60000]\n",
      "loss: 1.326444  [51200/60000]\n",
      "loss: 1.212976  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.183348 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 1.280733  [    0/60000]\n",
      "loss: 1.309737  [ 6400/60000]\n",
      "loss: 1.041034  [12800/60000]\n",
      "loss: 1.013646  [19200/60000]\n",
      "loss: 1.223369  [25600/60000]\n",
      "loss: 0.907994  [32000/60000]\n",
      "loss: 1.287504  [38400/60000]\n",
      "loss: 1.231430  [44800/60000]\n",
      "loss: 1.326008  [51200/60000]\n",
      "loss: 1.212807  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.183102 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 1.280638  [    0/60000]\n",
      "loss: 1.309428  [ 6400/60000]\n",
      "loss: 1.040906  [12800/60000]\n",
      "loss: 1.013200  [19200/60000]\n",
      "loss: 1.223068  [25600/60000]\n",
      "loss: 0.907729  [32000/60000]\n",
      "loss: 1.287199  [38400/60000]\n",
      "loss: 1.230960  [44800/60000]\n",
      "loss: 1.325497  [51200/60000]\n",
      "loss: 1.212639  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.182856 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 1.280537  [    0/60000]\n",
      "loss: 1.309106  [ 6400/60000]\n",
      "loss: 1.040745  [12800/60000]\n",
      "loss: 1.012812  [19200/60000]\n",
      "loss: 1.222704  [25600/60000]\n",
      "loss: 0.907500  [32000/60000]\n",
      "loss: 1.286935  [38400/60000]\n",
      "loss: 1.230489  [44800/60000]\n",
      "loss: 1.324963  [51200/60000]\n",
      "loss: 1.212439  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.182618 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 1.280435  [    0/60000]\n",
      "loss: 1.308753  [ 6400/60000]\n",
      "loss: 1.040636  [12800/60000]\n",
      "loss: 1.012380  [19200/60000]\n",
      "loss: 1.222328  [25600/60000]\n",
      "loss: 0.907248  [32000/60000]\n",
      "loss: 1.286677  [38400/60000]\n",
      "loss: 1.230032  [44800/60000]\n",
      "loss: 1.324550  [51200/60000]\n",
      "loss: 1.212274  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.182366 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 1.280365  [    0/60000]\n",
      "loss: 1.308427  [ 6400/60000]\n",
      "loss: 1.040471  [12800/60000]\n",
      "loss: 1.012332  [19200/60000]\n",
      "loss: 1.221969  [25600/60000]\n",
      "loss: 0.906806  [32000/60000]\n",
      "loss: 1.286373  [38400/60000]\n",
      "loss: 1.229680  [44800/60000]\n",
      "loss: 1.324033  [51200/60000]\n",
      "loss: 1.212151  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.182137 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 1.280312  [    0/60000]\n",
      "loss: 1.307757  [ 6400/60000]\n",
      "loss: 1.040318  [12800/60000]\n",
      "loss: 1.011879  [19200/60000]\n",
      "loss: 1.221567  [25600/60000]\n",
      "loss: 0.906558  [32000/60000]\n",
      "loss: 1.286065  [38400/60000]\n",
      "loss: 1.229250  [44800/60000]\n",
      "loss: 1.323634  [51200/60000]\n",
      "loss: 1.211951  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.181904 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 1.280219  [    0/60000]\n",
      "loss: 1.307485  [ 6400/60000]\n",
      "loss: 1.040199  [12800/60000]\n",
      "loss: 1.011526  [19200/60000]\n",
      "loss: 1.221161  [25600/60000]\n",
      "loss: 0.906348  [32000/60000]\n",
      "loss: 1.285784  [38400/60000]\n",
      "loss: 1.228206  [44800/60000]\n",
      "loss: 1.323266  [51200/60000]\n",
      "loss: 1.211763  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.181679 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 1.280202  [    0/60000]\n",
      "loss: 1.307176  [ 6400/60000]\n",
      "loss: 1.040025  [12800/60000]\n",
      "loss: 1.011142  [19200/60000]\n",
      "loss: 1.220513  [25600/60000]\n",
      "loss: 0.906056  [32000/60000]\n",
      "loss: 1.285571  [38400/60000]\n",
      "loss: 1.227958  [44800/60000]\n",
      "loss: 1.322854  [51200/60000]\n",
      "loss: 1.211533  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.181455 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 1.280182  [    0/60000]\n",
      "loss: 1.306836  [ 6400/60000]\n",
      "loss: 1.039860  [12800/60000]\n",
      "loss: 1.010737  [19200/60000]\n",
      "loss: 1.220189  [25600/60000]\n",
      "loss: 0.905757  [32000/60000]\n",
      "loss: 1.285327  [38400/60000]\n",
      "loss: 1.227565  [44800/60000]\n",
      "loss: 1.322415  [51200/60000]\n",
      "loss: 1.211259  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.181226 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 1.280160  [    0/60000]\n",
      "loss: 1.306534  [ 6400/60000]\n",
      "loss: 1.039713  [12800/60000]\n",
      "loss: 1.010357  [19200/60000]\n",
      "loss: 1.219755  [25600/60000]\n",
      "loss: 0.905566  [32000/60000]\n",
      "loss: 1.285057  [38400/60000]\n",
      "loss: 1.227132  [44800/60000]\n",
      "loss: 1.321985  [51200/60000]\n",
      "loss: 1.211076  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.181002 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 1.280136  [    0/60000]\n",
      "loss: 1.306205  [ 6400/60000]\n",
      "loss: 1.039580  [12800/60000]\n",
      "loss: 1.010003  [19200/60000]\n",
      "loss: 1.219351  [25600/60000]\n",
      "loss: 0.905354  [32000/60000]\n",
      "loss: 1.284803  [38400/60000]\n",
      "loss: 1.226690  [44800/60000]\n",
      "loss: 1.321571  [51200/60000]\n",
      "loss: 1.210901  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.180773 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 1.280103  [    0/60000]\n",
      "loss: 1.305878  [ 6400/60000]\n",
      "loss: 1.039420  [12800/60000]\n",
      "loss: 1.009529  [19200/60000]\n",
      "loss: 1.219049  [25600/60000]\n",
      "loss: 0.905132  [32000/60000]\n",
      "loss: 1.284553  [38400/60000]\n",
      "loss: 1.226392  [44800/60000]\n",
      "loss: 1.321116  [51200/60000]\n",
      "loss: 1.210653  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.180542 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 1.280122  [    0/60000]\n",
      "loss: 1.305526  [ 6400/60000]\n",
      "loss: 1.039263  [12800/60000]\n",
      "loss: 1.009132  [19200/60000]\n",
      "loss: 1.218705  [25600/60000]\n",
      "loss: 0.904955  [32000/60000]\n",
      "loss: 1.284283  [38400/60000]\n",
      "loss: 1.225993  [44800/60000]\n",
      "loss: 1.320794  [51200/60000]\n",
      "loss: 1.210427  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.180318 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 1.280144  [    0/60000]\n",
      "loss: 1.305219  [ 6400/60000]\n",
      "loss: 1.039089  [12800/60000]\n",
      "loss: 1.008784  [19200/60000]\n",
      "loss: 1.218378  [25600/60000]\n",
      "loss: 0.904719  [32000/60000]\n",
      "loss: 1.283948  [38400/60000]\n",
      "loss: 1.225498  [44800/60000]\n",
      "loss: 1.320411  [51200/60000]\n",
      "loss: 1.210237  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.180101 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 1.280117  [    0/60000]\n",
      "loss: 1.304855  [ 6400/60000]\n",
      "loss: 1.038959  [12800/60000]\n",
      "loss: 1.008481  [19200/60000]\n",
      "loss: 1.218019  [25600/60000]\n",
      "loss: 0.904546  [32000/60000]\n",
      "loss: 1.283604  [38400/60000]\n",
      "loss: 1.225049  [44800/60000]\n",
      "loss: 1.319986  [51200/60000]\n",
      "loss: 1.209981  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.179883 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 1.280011  [    0/60000]\n",
      "loss: 1.304556  [ 6400/60000]\n",
      "loss: 1.038250  [12800/60000]\n",
      "loss: 1.008141  [19200/60000]\n",
      "loss: 1.217710  [25600/60000]\n",
      "loss: 0.904319  [32000/60000]\n",
      "loss: 1.283334  [38400/60000]\n",
      "loss: 1.224618  [44800/60000]\n",
      "loss: 1.319559  [51200/60000]\n",
      "loss: 1.209783  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.179663 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 1.279967  [    0/60000]\n",
      "loss: 1.304248  [ 6400/60000]\n",
      "loss: 1.038070  [12800/60000]\n",
      "loss: 1.007827  [19200/60000]\n",
      "loss: 1.217368  [25600/60000]\n",
      "loss: 0.904108  [32000/60000]\n",
      "loss: 1.283075  [38400/60000]\n",
      "loss: 1.224188  [44800/60000]\n",
      "loss: 1.319127  [51200/60000]\n",
      "loss: 1.209749  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.179445 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 1.279919  [    0/60000]\n",
      "loss: 1.303941  [ 6400/60000]\n",
      "loss: 1.037923  [12800/60000]\n",
      "loss: 1.007522  [19200/60000]\n",
      "loss: 1.217000  [25600/60000]\n",
      "loss: 0.903831  [32000/60000]\n",
      "loss: 1.282833  [38400/60000]\n",
      "loss: 1.223738  [44800/60000]\n",
      "loss: 1.318692  [51200/60000]\n",
      "loss: 1.209575  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.179239 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 1.279814  [    0/60000]\n",
      "loss: 1.303693  [ 6400/60000]\n",
      "loss: 1.037787  [12800/60000]\n",
      "loss: 1.007230  [19200/60000]\n",
      "loss: 1.216614  [25600/60000]\n",
      "loss: 0.903565  [32000/60000]\n",
      "loss: 1.282592  [38400/60000]\n",
      "loss: 1.223333  [44800/60000]\n",
      "loss: 1.318334  [51200/60000]\n",
      "loss: 1.209411  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.179012 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 1.279744  [    0/60000]\n",
      "loss: 1.303390  [ 6400/60000]\n",
      "loss: 1.037700  [12800/60000]\n",
      "loss: 1.006867  [19200/60000]\n",
      "loss: 1.216290  [25600/60000]\n",
      "loss: 0.903384  [32000/60000]\n",
      "loss: 1.282340  [38400/60000]\n",
      "loss: 1.222938  [44800/60000]\n",
      "loss: 1.317876  [51200/60000]\n",
      "loss: 1.209246  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.178801 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 1.279661  [    0/60000]\n",
      "loss: 1.303053  [ 6400/60000]\n",
      "loss: 1.037576  [12800/60000]\n",
      "loss: 1.006528  [19200/60000]\n",
      "loss: 1.215930  [25600/60000]\n",
      "loss: 0.903101  [32000/60000]\n",
      "loss: 1.282085  [38400/60000]\n",
      "loss: 1.222558  [44800/60000]\n",
      "loss: 1.317466  [51200/60000]\n",
      "loss: 1.209049  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.178589 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 1.279582  [    0/60000]\n",
      "loss: 1.302753  [ 6400/60000]\n",
      "loss: 1.037444  [12800/60000]\n",
      "loss: 1.006189  [19200/60000]\n",
      "loss: 1.215570  [25600/60000]\n",
      "loss: 0.902814  [32000/60000]\n",
      "loss: 1.281809  [38400/60000]\n",
      "loss: 1.222207  [44800/60000]\n",
      "loss: 1.317082  [51200/60000]\n",
      "loss: 1.208879  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.178385 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b537f-faff-434f-88f5-43b809708266",
   "metadata": {},
   "source": [
    "# 6. 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbcd6b69-1fbd-4847-9d27-5b0a49157852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0af6fe4d-35b0-4187-bdda-a13e6ffcd88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBinaryIO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIO\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpickle_module\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mmodule\u001b[0m \u001b[1;34m'pickle'\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;34m'C:\\\\Users\\\\user\\\\anaconda3\\\\envs\\\\bda2021\\\\lib\\\\pickle.py'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\n",
       "\n",
       "Saves an object to a disk file.\n",
       "\n",
       "See also: :ref:`saving-loading-tensors`\n",
       "\n",
       "Args:\n",
       "    obj: saved object\n",
       "    f: a file-like object (has to implement write and flush) or a string or\n",
       "       os.PathLike object containing a file name\n",
       "    pickle_module: module used for pickling metadata and objects\n",
       "    pickle_protocol: can be specified to override the default protocol\n",
       "\n",
       ".. note::\n",
       "    A common PyTorch convention is to save tensors using .pt file extension.\n",
       "\n",
       ".. note::\n",
       "    PyTorch preserves storage sharing across serialization. See\n",
       "    :ref:`preserve-storage-sharing` for more details.\n",
       "\n",
       ".. note::\n",
       "    The 1.6 release of PyTorch switched ``torch.save`` to use a new\n",
       "    zipfile-based file format. ``torch.load`` still retains the ability to\n",
       "    load files in the old format. If for any reason you want ``torch.save``\n",
       "    to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.\n",
       "\n",
       "Example:\n",
       "    >>> # Save to file\n",
       "    >>> x = torch.tensor([0, 1, 2, 3, 4])\n",
       "    >>> torch.save(x, 'tensor.pt')\n",
       "    >>> # Save to io.BytesIO buffer\n",
       "    >>> buffer = io.BytesIO()\n",
       "    >>> torch.save(x, buffer)\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\user\\anaconda3\\envs\\bda2021\\lib\\site-packages\\torch\\serialization.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae959ff3-c777-4b4a-840c-07d346249de2",
   "metadata": {},
   "source": [
    "# 7. 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08f1fe99-10be-4914-a528-b23b29a154b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954babac-0e87-4b71-ae26-95812994bcca",
   "metadata": {},
   "source": [
    "## 불러온 모델을 이용한 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b510eb0-a538-4745-b2c6-3e53ee2adf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09713635-7337-4178-ba7d-433b06e7d837",
   "metadata": {},
   "source": [
    "## 테스트 데이터 시각화 (tensor를 PILImage로 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e48c465-d718-46ca-8326-51334d39f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = ToPILImage()\n",
    "img_t = tf(test_data[0][0])\n",
    "img_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "163b5a08-3bce-4f16-8dbb-c7ace232ed75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fca3197340>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2dd994a2-2ba9-4176-8c29-59ceaa386650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be6c4f-383f-4155-9ff7-4113dba07d5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (3주차-2) 11월26일\n",
    "- 주제: 텐서\n",
    "- 작성자: 윤도현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e79ee-17a0-4086-9b3a-53602871057f",
   "metadata": {},
   "source": [
    "pytorch에서는 텐서를 사용하여 모델의 입력,출력 그리고 모델의 매개변수들을 encode 한다. (encoding이란 사람의 언어를 컴퓨터가 이해할 수 있는 언어로 변환하는 과정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6ed1934-144a-4e53-b0c1-338c33862376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c64b70-74dc-48f6-ab47-5d82fd275016",
   "metadata": {},
   "source": [
    "# 1. 텐서 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09918fe5-cef9-41c4-879b-159c4ea13eca",
   "metadata": {},
   "source": [
    "`-` 데이터로부터 직접 텐서 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d328690-eb33-4d82-bf0c-46bb343ddc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4ef44e6-b7f1-4386-97d1-c71d5bf1dfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38495f-79fe-46d3-8a4f-7266faac4cc3",
   "metadata": {},
   "source": [
    "`-` Numpy 배열로부터 텐서 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2d7ad39-8f65-4c61-be2e-386c6c503df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33cd1163-7bac-454a-b032-4596a0d1e484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411bff66-1fa8-4523-bd0d-728b49468bec",
   "metadata": {},
   "source": [
    "`-` 다른 텐서로부터 텐서 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e858c3a-e9c7-4771-84e6-a46ad98a37d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9338, 0.0484],\n",
      "        [0.2330, 0.4096]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786c6a5-8004-45ac-b92a-928dc2782a77",
   "metadata": {},
   "source": [
    "`-` 무작위(random) 또는 상수(constant)값 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c090b910-9c51-499c-af2f-6b81f028291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0270, 0.6254, 0.6863],\n",
       "         [0.1242, 0.6509, 0.8563]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "rand_tensor, ones_tensor, zeros_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98c349-2a03-4a73-baf3-4a086d1aa390",
   "metadata": {},
   "source": [
    "# 2. 텐서의 속성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a65f15-f25e-403c-93c1-5a0dd05b4f61",
   "metadata": {},
   "source": [
    "`-` 텐서의 속성은 텐서의 모양, 자료형, 어느 장치에 저장되는지 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bce4579-8132-4ab9-b27b-dea84e4c13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor= torch.rand(3,4)\n",
    "\n",
    "tensor.shape, tensor.dtype, tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780308cb-ad3b-46df-8067-98a477405eec",
   "metadata": {},
   "source": [
    "# 3. 텐서 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886ec81-e9b6-437a-8d70-210c8595b7dd",
   "metadata": {},
   "source": [
    "`-` 전치(Transposing), 인덱싱(indexing), 슬라이싱(slicing), 수학계산, 선형 대수, 임의 샘플링 등 100가지 이상의 텐서 연산 가능\n",
    "\n",
    "`-` 기본적으로 텐서는 cpu에 생성된다.\n",
    "\n",
    "`-` .to api를 사용하면 gpu로 텐서를 이동시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b8ad4-6d8d-4504-a167-ab3a8d5bf0be",
   "metadata": {},
   "source": [
    "## cpu에 있는 텐서를 gpu로 보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1fbd3a16-5559-4763-bbac-74d44ceefac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a5e75-0add-4463-8b01-e04d0417edd5",
   "metadata": {},
   "source": [
    "## numpy식의 표준 인덱싱과 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "521cc180-db19-42e8-af9a-42456e876b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(tensor)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab6877-4006-4caf-aad5-81a444172d6f",
   "metadata": {},
   "source": [
    "## 텐서 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a9041b1-37b3-486b-9717-145ae61f1d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f51a3-b05b-4ad8-a9f8-b05f1580acb9",
   "metadata": {},
   "source": [
    "## 산술 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0143688-918c-4743-a13f-c2073d77a10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 텐서 간의 행렬 곱\n",
    "y1 = tensor@ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor,tensor.T,out=y3)\n",
    "\n",
    "#요소별 곱\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor,tensor,out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4794009-4705-47e4-aa5c-0308ffaa054f",
   "metadata": {},
   "source": [
    "## 단일요소 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19da20f-c9d6-4a83-866b-31a59b6ad471",
   "metadata": {},
   "source": [
    "`-` 텐서의 모든값이 하나로 집계(aggregate)되어 요소가 하나인 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25724d7d-a787-47c6-8f5c-3e7d38f1f156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.0, float)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "\n",
    "agg_item, type(agg_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ab153-16c8-4132-b5a6-9d67cefb45a2",
   "metadata": {},
   "source": [
    "## 바꿔치기 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ff5c3-d0c1-4160-b648-4bdc0e09de47",
   "metadata": {},
   "source": [
    "`-` 연산결과를 피연산자에 저장하는 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15222918-e14f-4cb3-98e3-01bbf8562bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a6b3fb0d-9b76-4c72-bf75-18a4bd75ebdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 5., 6., 6.],\n",
       "        [6., 5., 6., 6.],\n",
       "        [6., 5., 6., 6.],\n",
       "        [6., 5., 6., 6.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.add_(5)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbb097-93ce-414a-b03a-e2d9af015f1b",
   "metadata": {},
   "source": [
    "# 4. Numpy 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95147e30-ea26-4608-95fa-996f3ff5ec63",
   "metadata": {},
   "source": [
    "`-` cpu 상의 텐서와 Numpy 배열은 메모리 공간을 공유하기 때문에 하나만 변경해도 다른 하나도 변경된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49b07599-6f5b-4ba3-bc11-b1f0fb9d7eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t= torch.ones(5)\n",
    "n= t.numpy()\n",
    "\n",
    "t, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3918a907-0652-47ae-975f-51f9dbf936a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2.]), array([2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.add_(1)\n",
    "t, n "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc426f-0cf6-4f31-bbe5-6c72061523c2",
   "metadata": {},
   "source": [
    "## Numpy 배열을 텐서로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "88f72601-f443-4e2a-a912-c16a15a7282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b92c96cd-6683-4c91-b3b3-00b6fd82cb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.], dtype=torch.float64),\n",
       " array([1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(n,1)\n",
    "t, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb8b4349-5b11-44a7-99fd-8654c7e238e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mCall signature:\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mType:\u001b[0m            ufunc\n",
       "\u001b[1;31mString form:\u001b[0m     <ufunc 'add'>\n",
       "\u001b[1;31mFile:\u001b[0m            c:\\users\\user\\anaconda3\\envs\\bda2021\\lib\\site-packages\\numpy\\__init__.py\n",
       "\u001b[1;31mDocstring:\u001b[0m      \n",
       "add(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
       "\n",
       "Add arguments element-wise.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "x1, x2 : array_like\n",
       "    The arrays to be added.\n",
       "    If ``x1.shape != x2.shape``, they must be broadcastable to a common\n",
       "    shape (which becomes the shape of the output).\n",
       "out : ndarray, None, or tuple of ndarray and None, optional\n",
       "    A location into which the result is stored. If provided, it must have\n",
       "    a shape that the inputs broadcast to. If not provided or None,\n",
       "    a freshly-allocated array is returned. A tuple (possible only as a\n",
       "    keyword argument) must have length equal to the number of outputs.\n",
       "where : array_like, optional\n",
       "    This condition is broadcast over the input. At locations where the\n",
       "    condition is True, the `out` array will be set to the ufunc result.\n",
       "    Elsewhere, the `out` array will retain its original value.\n",
       "    Note that if an uninitialized `out` array is created via the default\n",
       "    ``out=None``, locations within it where the condition is False will\n",
       "    remain uninitialized.\n",
       "**kwargs\n",
       "    For other keyword-only arguments, see the\n",
       "    :ref:`ufunc docs <ufuncs.kwargs>`.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "add : ndarray or scalar\n",
       "    The sum of `x1` and `x2`, element-wise.\n",
       "    This is a scalar if both `x1` and `x2` are scalars.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Equivalent to `x1` + `x2` in terms of array broadcasting.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> np.add(1.0, 4.0)\n",
       "5.0\n",
       ">>> x1 = np.arange(9.0).reshape((3, 3))\n",
       ">>> x2 = np.arange(3.0)\n",
       ">>> np.add(x1, x2)\n",
       "array([[  0.,   2.,   4.],\n",
       "       [  3.,   5.,   7.],\n",
       "       [  6.,   8.,  10.]])\n",
       "\n",
       "The ``+`` operator can be used as a shorthand for ``np.add`` on ndarrays.\n",
       "\n",
       ">>> x1 = np.arange(9.0).reshape((3, 3))\n",
       ">>> x2 = np.arange(3.0)\n",
       ">>> x1 + x2\n",
       "array([[ 0.,  2.,  4.],\n",
       "       [ 3.,  5.,  7.],\n",
       "       [ 6.,  8., 10.]])\n",
       "\u001b[1;31mClass docstring:\u001b[0m\n",
       "Functions that operate element by element on whole arrays.\n",
       "\n",
       "To see the documentation for a specific ufunc, use `info`.  For\n",
       "example, ``np.info(np.sin)``.  Because ufuncs are written in C\n",
       "(for speed) and linked into Python with NumPy's ufunc facility,\n",
       "Python's help() function finds this page whenever help() is called\n",
       "on a ufunc.\n",
       "\n",
       "A detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n",
       "\n",
       "**Calling ufuncs:** ``op(*x[, out], where=True, **kwargs)``\n",
       "\n",
       "Apply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n",
       "\n",
       "The broadcasting rules are:\n",
       "\n",
       "* Dimensions of length 1 may be prepended to either array.\n",
       "* Arrays may be repeated along dimensions of length 1.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "*x : array_like\n",
       "    Input arrays.\n",
       "out : ndarray, None, or tuple of ndarray and None, optional\n",
       "    Alternate array object(s) in which to put the result; if provided, it\n",
       "    must have a shape that the inputs broadcast to. A tuple of arrays\n",
       "    (possible only as a keyword argument) must have length equal to the\n",
       "    number of outputs; use None for uninitialized outputs to be\n",
       "    allocated by the ufunc.\n",
       "where : array_like, optional\n",
       "    This condition is broadcast over the input. At locations where the\n",
       "    condition is True, the `out` array will be set to the ufunc result.\n",
       "    Elsewhere, the `out` array will retain its original value.\n",
       "    Note that if an uninitialized `out` array is created via the default\n",
       "    ``out=None``, locations within it where the condition is False will\n",
       "    remain uninitialized.\n",
       "**kwargs\n",
       "    For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "r : ndarray or tuple of ndarray\n",
       "    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n",
       "    provided, it will be returned. If not, `r` will be allocated and\n",
       "    may contain uninitialized values. If the function has more than one\n",
       "    output, then the result will be a tuple of arrays.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722c0f0-bf1e-4965-8778-62889befb758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f96245-2d47-4c85-b51f-45defc452d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cb5d8-f877-4965-a46c-3d418d6d7633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec6e48-7a9c-455e-b826-2566e49b47f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364dc5b-54d5-4330-9589-59ad29e73d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3799ca0-d716-4db6-bcb3-5b1fd2533fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b4c7d94-28c6-4434-8f62-ea965c624b88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (3주차-3) 11월26일\n",
    "- 주제: Dataset과 Dataloader\n",
    "- 작성자: 윤도현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3ca61-bc62-44cb-85e6-f98a231e4fba",
   "metadata": {},
   "source": [
    "# 1. 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b7c9bc2-6c64-4b20-be7d-e95d806df324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd3e9a-9bfd-4759-b01f-b3ab879ac43d",
   "metadata": {},
   "source": [
    "## Dataset과 DataLoader 함수 사용하는 이유??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c7165-574e-47f3-8319-87d0024e8d29",
   "metadata": {},
   "source": [
    "### 결론부터 말하자면 \"Customizing\" 하기 위해서\n",
    "### 딥러닝 모델을 학습시킬때 데이터셋을 가져와야 하는데 한번에 모든 데이터를 가져오면 gpu 메모리가 터져버리는 일이 생긴다.\n",
    "### 그래서 batch를 나눠서 모델에 입력을 해주는데 어떤 데이터를 가져오고, 어떤 형태로 가져오고, 한 번에 얼마나 가져올지 customzing 하기 위해 torch에 dataset과 dataloader를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1dfebce4-1e6c-4be0-99ce-a628d1151fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924638a3-ddf1-4e08-9fe3-f12d3fbaefea",
   "metadata": {},
   "source": [
    "# 2. 데이터셋 직접 접근 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7eb74202-368f-4566-a097-d813ac270b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7f264-4f85-4645-b9af-c6949aa41e10",
   "metadata": {},
   "source": [
    "# 3. 사용자 정의 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fa3ee-bbcd-497e-b8a5-40fd326f6716",
   "metadata": {},
   "source": [
    "`-` 우리가 가장 관심있고 배워야하는 부분! \n",
    "\n",
    "`-` 사용자 정의 dataset 클래스는 반드시 3개의 함수를 포함해야 함(__ init __, __ len __ and __ getitem __)\n",
    "\n",
    "`-` FashionMNIST 이미지들은 img_dir 디렉토리에 저장되고 정답은 annotations_file csv파일에 별도로 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b19229cb-7e55-4646-b2f6-f488628a35e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541b146-3eac-4ef3-838c-8c551536af37",
   "metadata": {},
   "source": [
    "## Custom Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "042ee3ff-818c-494c-9f51-c28aedeff4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx,1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label= self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3b220-2c2d-439c-99a2-41e5ebd7f3ad",
   "metadata": {},
   "source": [
    "## __ init __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca7a38-d734-469a-a223-81bbc75db1f9",
   "metadata": {},
   "source": [
    "`-` initialize 라는 이름 그대로 인스턴스(객체)를 초기화\n",
    "\n",
    "`-` __ init __ 함수는 Dataset 객체가 생성될 떄 한번만 실행된다\n",
    "\n",
    "`-` 여기서는 이미지와 주석파일(annotation file)이 포함된 디렉토리와 두가지 Transform을 초기화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9d971-87c8-4acc-b8a0-47e2ee9fbceb",
   "metadata": {},
   "source": [
    "## __ len __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458382b-646e-4c69-a8be-73c72d412efa",
   "metadata": {},
   "source": [
    "`-` 데이터셋의 샘플 개수를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5ce13-4b5f-4d4e-9c02-2403c23fcfbb",
   "metadata": {},
   "source": [
    "## __ getitem __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4938d7-332a-4585-a167-1f7e37c0fa2d",
   "metadata": {},
   "source": [
    "`-` __ getitem __ 함수는 주어진 인덱스 idx에 해당하는 샘플을 데이터셋에서 불러오고 반환한다.\n",
    "\n",
    "`-` 인덱스를 기반으로 디스크에서 이미지의 위치를 식별하고, read_image 함수를 사용하여 이미지를 텐서로 변환하고, self.img_labels의 csv 데이터로부터 해당하는 정답(label)을 가져오고, 변형 함수들을 호출한 뒤, 텐서 이미지와 라벨을 python dictionary 형으로 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a5ab99-22ff-45e8-9eac-632c9678dd9d",
   "metadata": {},
   "source": [
    "# 4. DataLoader로 학습용 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994cf66-f3f3-4516-bb04-2c00a336f485",
   "metadata": {},
   "source": [
    "`-` 앞서 설명한 Dataset은 데이터셋의 __특징(feature)과__ 하나의 샘플에 __정답(label)을__ 지정하는 일을 동시에 했음\n",
    "\n",
    "`-` 모델을 학습할때 일반적으로 데이터를 batch로 묶어 전달하고, 매 epoch마다 데이터를 다시 mix해서 과적합(overfitting)을 방지했다.\n",
    "\n",
    "`-` DataLoader 함수는 이러한 많은 작업을 하나의 api로 가능하도록 해주는 아주 좋은놈! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "227ab21e-87b6-431f-85d2-3a024bf630aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "\n",
    "train_dl = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef98e8-a53e-4c18-a3e1-bfb94da07395",
   "metadata": {},
   "source": [
    "# 5. DataLoader를 통해 batch로 묶고 iteration 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a622915-f1f9-4f25-9354-7866344a8a5c",
   "metadata": {},
   "source": [
    "- 만약 데이터셋의 크기가 1024이고 batch_size= 32라고 가정해보자  \n",
    "\n",
    "- 하나의 batch는 32개의 데이터를 포함하고 총 32개의 batch로 묶이게 된다.\n",
    "\n",
    "- 전체 데이터셋을 학습할때 batch로 묶어 학습하게 되면 1 epoch은 32번의 iteration으로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "39f70293-0693-4406-af7f-161dec58e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    }
   ],
   "source": [
    "# 이미지와 정답(label)을 표시합니다.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69662592-9cb9-43b0-ada7-fdee9a9f22b1",
   "metadata": {},
   "source": [
    "# 6. 변형(Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78281d-069e-4724-ad0e-e02d309c32e6",
   "metadata": {},
   "source": [
    "`-` 데이터가 항상 바로 모델에 학습시킬 수 있도록 깔끔하게 주어지는 경우는 거의 없다.\n",
    "\n",
    "`-` 변형(transform)을 해서 데이터를 조작하고 학습에 적합하게 만드는 과정이 필수!! \n",
    "\n",
    "`-` torchvision에서 특징(feature)을 변경해주는 transform 함수, 정답(label)을 변경해주는 target_transform 함수를 제공한다.\n",
    "\n",
    "`-` FashionMNIST 데이터셋의 특징(feature)은 PIL Image 형식이고, 정답(label)은 정수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5506e27-9f64-44d5-9222-dfcf6d384874",
   "metadata": {},
   "source": [
    "## 이대로 학습 ? == 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04935597-e7de-4a49-8bfe-bf801b9e11ea",
   "metadata": {},
   "source": [
    "`-` 특징을 정규화된 텐서 형태로 변환 $  \\to  $ torchvision.transforms.ToTensor\n",
    "\n",
    "`-` 정답을 one-hot encoding $  \\to  $ torchvision.transform.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "91a8fe1d-ac4f-4069-82a7-4b8450bc49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea51d5-254e-4611-9e26-2bca8085df16",
   "metadata": {},
   "source": [
    "## 이게 뭐한거냐?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f9f35-02ce-4597-b9f4-078f5317f303",
   "metadata": {},
   "source": [
    "## ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9818a36-2958-43a6-82f1-d0e61ecc9825",
   "metadata": {},
   "source": [
    "`-` ToTensor는 PIL Image나 Numpy 배열을 FloatTensor로 변환하고, 이미지의 픽셀의 크기 값을 (0~1) 범위로 비례 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c2ea7-20ff-4bbc-9997-6be316205495",
   "metadata": {},
   "source": [
    "## Lambda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b799c-7f06-4f82-adb8-9a4ef7abefb9",
   "metadata": {},
   "source": [
    "`-` Lambda는 정수를 one-hot encoding 해준다.\n",
    "\n",
    "`-` 먼저 데이터셋 정답개수 크기의 zero tensor를 만든다.\n",
    "\n",
    "`-` 그다음 scatter_를 호출하여 주어진 정답 y에 해당하는 인덱스에 value=1을 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "37563a74-90ea-4164-b5c4-67dcb081bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91a4e1-2f68-4534-a37e-a6284e6e7126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
